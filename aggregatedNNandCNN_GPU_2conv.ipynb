{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the GPU!\n"
     ]
    }
   ],
   "source": [
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using the GPU!\")\n",
    "else:\n",
    "    print(\"WARNING: Could not find GPU! Using CPU only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample size of training set is:  3556\n",
      "The sample size of testing set is:  889\n"
     ]
    }
   ],
   "source": [
    "x_train_nhts = np.load(\"data/x_train_nhts.npy\")\n",
    "x_test_nhts = np.load(\"data/x_test_nhts.npy\")\n",
    " \n",
    "x_train_images = np.load(\"data/x_train_images.npy\")\n",
    "x_test_images = np.load(\"data/x_test_images.npy\")\n",
    "  \n",
    "\n",
    "y_train = np.load(\"data/y_train.npy\")\n",
    "y_test = np.load(\"data/y_test.npy\")\n",
    "print(\"The sample size of training set is: \", x_train_nhts.shape[0])\n",
    "print(\"The sample size of testing set is: \", x_test_nhts.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "x_dim 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2    0.339145\n",
       "1    0.323960\n",
       "3    0.249438\n",
       "0    0.087458\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bridge numpy to torch\n",
    "x_train_nhts_torch = torch.as_tensor(x_train_nhts).float() # specify floats for the inputs\n",
    "x_train_images_torch = torch.as_tensor(x_train_images).float()\n",
    "x_test_nhts_torch = torch.as_tensor(x_test_nhts).float()\n",
    "x_test_images_torch = torch.as_tensor(x_test_images).float()\n",
    "y_train_torch = torch.as_tensor(y_train[:,0])\n",
    "y_test_torch = torch.as_tensor(y_test[:,0])\n",
    "n_train = x_train_nhts.shape[0]\n",
    "n_test = x_test_nhts.shape[0]\n",
    "# inputs: x_train_nhts, x_train_images, x_test_nhts, x_test_images, y_train, and y_test; \n",
    "K = len(np.unique(y_train))\n",
    "print(K)\n",
    "x_dim = x_train_nhts.shape[1]\n",
    "print(\"x_dim\", x_dim)\n",
    "# \n",
    "pd.value_counts(y_train[:,0])/y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class combinedNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(combinedNN, self).__init__()\n",
    "        # To-Do: need to have more channels for higher accuracy. \n",
    "        self.conv1 = nn.Conv2d(in_channels=4, out_channels=5, kernel_size=4, padding=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=5, out_channels=10, kernel_size=4)\n",
    "        # Question: Why is this 48*48 correct?\n",
    "        self.fcCNN1 = nn.Linear(in_features=10 * 48 * 48, out_features=100)\n",
    "        self.fcCNN2 = nn.Linear(in_features=100, out_features=100)\n",
    "        # \n",
    "        self.fcNN1 = nn.Linear(x_dim, 100)        \n",
    "        self.fcNN2 = nn.Linear(100, 100)\n",
    "        self.fcNN3 = nn.Linear(100, 100)\n",
    "        #\n",
    "        self.fcNN = nn.Linear(200, K)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, image, nhts):\n",
    "        #image\n",
    "        outCNN = F.relu(self.conv1(image))\n",
    "        outCNN = F.max_pool2d(outCNN, 2)\n",
    "        outCNN = F.relu(self.conv2(outCNN))\n",
    "        outCNN = F.max_pool2d(outCNN, 2)\n",
    "        outCNN = outCNN.reshape(outCNN.size(0), -1)\n",
    "        outCNN = F.relu(self.fcCNN1(outCNN))\n",
    "        outCNN = F.relu(self.fcCNN2(outCNN))\n",
    "        #nhts\n",
    "        nhts = self.fcNN1(nhts)\n",
    "        nhts = nhts.relu()\n",
    "        nhts = self.fcNN2(nhts)\n",
    "        nhts = nhts.relu()\n",
    "        nhts = self.fcNN3(nhts)\n",
    "        #combined\n",
    "        out = self.fcNN(torch.cat((nhts,outCNN),1)) #200*4\n",
    "        out = self.softmax(out) # 200*4    \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the data\n",
    "x_train_images_norm_torch = x_train_images_torch/255.0\n",
    "x_test_images_norm_torch = x_test_images_torch/255.0\n",
    "# \n",
    "combined_net = combinedNN().float().to(device)\n",
    "optim = torch.optim.Adam(combined_net.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# \n",
    "n_epoches = 150 # To-Do: need more epoches.\n",
    "batch_size = 200 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "0 200\n",
      "0 400\n",
      "0 600\n",
      "0 800\n",
      "0 1000\n",
      "0 1200\n",
      "0 1400\n",
      "0 1600\n",
      "0 1800\n",
      "0 2000\n",
      "0 2200\n",
      "0 2400\n",
      "0 2600\n",
      "0 2800\n",
      "0 3000\n",
      "0 3200\n",
      "0 3400\n",
      "Epoch 0: train loss: 1.3084290027618408; test loss: 1.2675464153289795\n",
      "Epoch 0: train accuracy: 0.4358830146231721; test accuracy: 0.43869516310461193\n",
      "1 0\n",
      "1 200\n",
      "1 400\n",
      "1 600\n",
      "1 800\n",
      "1 1000\n",
      "1 1200\n",
      "1 1400\n",
      "1 1600\n",
      "1 1800\n",
      "1 2000\n",
      "1 2200\n",
      "1 2400\n",
      "1 2600\n",
      "1 2800\n",
      "1 3000\n",
      "1 3200\n",
      "1 3400\n",
      "Epoch 1: train loss: 1.225966453552246; test loss: 1.253003478050232\n",
      "Epoch 1: train accuracy: 0.483970753655793; test accuracy: 0.4645669291338583\n",
      "2 0\n",
      "2 200\n",
      "2 400\n",
      "2 600\n",
      "2 800\n",
      "2 1000\n",
      "2 1200\n",
      "2 1400\n",
      "2 1600\n",
      "2 1800\n",
      "2 2000\n",
      "2 2200\n",
      "2 2400\n",
      "2 2600\n",
      "2 2800\n",
      "2 3000\n",
      "2 3200\n",
      "2 3400\n",
      "Epoch 2: train loss: 1.1912152767181396; test loss: 1.2118353843688965\n",
      "Epoch 2: train accuracy: 0.5438695163104612; test accuracy: 0.5163104611923509\n",
      "3 0\n",
      "3 200\n",
      "3 400\n",
      "3 600\n",
      "3 800\n",
      "3 1000\n",
      "3 1200\n",
      "3 1400\n",
      "3 1600\n",
      "3 1800\n",
      "3 2000\n",
      "3 2200\n",
      "3 2400\n",
      "3 2600\n",
      "3 2800\n",
      "3 3000\n",
      "3 3200\n",
      "3 3400\n",
      "Epoch 3: train loss: 1.1529314517974854; test loss: 1.1696364879608154\n",
      "Epoch 3: train accuracy: 0.5956130483689539; test accuracy: 0.5590551181102362\n",
      "4 0\n",
      "4 200\n",
      "4 400\n",
      "4 600\n",
      "4 800\n",
      "4 1000\n",
      "4 1200\n",
      "4 1400\n",
      "4 1600\n",
      "4 1800\n",
      "4 2000\n",
      "4 2200\n",
      "4 2400\n",
      "4 2600\n",
      "4 2800\n",
      "4 3000\n",
      "4 3200\n",
      "4 3400\n",
      "Epoch 4: train loss: 1.1045429706573486; test loss: 1.1378178596496582\n",
      "Epoch 4: train accuracy: 0.6448256467941508; test accuracy: 0.6197975253093363\n",
      "5 0\n",
      "5 200\n",
      "5 400\n",
      "5 600\n",
      "5 800\n",
      "5 1000\n",
      "5 1200\n",
      "5 1400\n",
      "5 1600\n",
      "5 1800\n",
      "5 2000\n",
      "5 2200\n",
      "5 2400\n",
      "5 2600\n",
      "5 2800\n",
      "5 3000\n",
      "5 3200\n",
      "5 3400\n",
      "Epoch 5: train loss: 1.074371576309204; test loss: 1.1008018255233765\n",
      "Epoch 5: train accuracy: 0.6706974128233971; test accuracy: 0.6366704161979753\n",
      "6 0\n",
      "6 200\n",
      "6 400\n",
      "6 600\n",
      "6 800\n",
      "6 1000\n",
      "6 1200\n",
      "6 1400\n",
      "6 1600\n",
      "6 1800\n",
      "6 2000\n",
      "6 2200\n",
      "6 2400\n",
      "6 2600\n",
      "6 2800\n",
      "6 3000\n",
      "6 3200\n",
      "6 3400\n",
      "Epoch 6: train loss: 1.054840326309204; test loss: 1.0779086351394653\n",
      "Epoch 6: train accuracy: 0.7050056242969629; test accuracy: 0.6614173228346457\n",
      "7 0\n",
      "7 200\n",
      "7 400\n",
      "7 600\n",
      "7 800\n",
      "7 1000\n",
      "7 1200\n",
      "7 1400\n",
      "7 1600\n",
      "7 1800\n",
      "7 2000\n",
      "7 2200\n",
      "7 2400\n",
      "7 2600\n",
      "7 2800\n",
      "7 3000\n",
      "7 3200\n",
      "7 3400\n",
      "Epoch 7: train loss: 1.0542646646499634; test loss: 1.0563104152679443\n",
      "Epoch 7: train accuracy: 0.7024746906636671; test accuracy: 0.6850393700787402\n",
      "8 0\n",
      "8 200\n",
      "8 400\n",
      "8 600\n",
      "8 800\n",
      "8 1000\n",
      "8 1200\n",
      "8 1400\n",
      "8 1600\n",
      "8 1800\n",
      "8 2000\n",
      "8 2200\n",
      "8 2400\n",
      "8 2600\n",
      "8 2800\n",
      "8 3000\n",
      "8 3200\n",
      "8 3400\n",
      "Epoch 8: train loss: 1.0403450727462769; test loss: 1.0451164245605469\n",
      "Epoch 8: train accuracy: 0.7241282339707537; test accuracy: 0.7007874015748031\n",
      "9 0\n",
      "9 200\n",
      "9 400\n",
      "9 600\n",
      "9 800\n",
      "9 1000\n",
      "9 1200\n",
      "9 1400\n",
      "9 1600\n",
      "9 1800\n",
      "9 2000\n",
      "9 2200\n",
      "9 2400\n",
      "9 2600\n",
      "9 2800\n",
      "9 3000\n",
      "9 3200\n",
      "9 3400\n",
      "Epoch 9: train loss: 1.0225794315338135; test loss: 1.0436135530471802\n",
      "Epoch 9: train accuracy: 0.7232845894263217; test accuracy: 0.7064116985376828\n",
      "10 0\n",
      "10 200\n",
      "10 400\n",
      "10 600\n",
      "10 800\n",
      "10 1000\n",
      "10 1200\n",
      "10 1400\n",
      "10 1600\n",
      "10 1800\n",
      "10 2000\n",
      "10 2200\n",
      "10 2400\n",
      "10 2600\n",
      "10 2800\n",
      "10 3000\n",
      "10 3200\n",
      "10 3400\n",
      "Epoch 10: train loss: 1.0238765478134155; test loss: 1.0283883810043335\n",
      "Epoch 10: train accuracy: 0.7438132733408324; test accuracy: 0.7131608548931384\n",
      "11 0\n",
      "11 200\n",
      "11 400\n",
      "11 600\n",
      "11 800\n",
      "11 1000\n",
      "11 1200\n",
      "11 1400\n",
      "11 1600\n",
      "11 1800\n",
      "11 2000\n",
      "11 2200\n",
      "11 2400\n",
      "11 2600\n",
      "11 2800\n",
      "11 3000\n",
      "11 3200\n",
      "11 3400\n",
      "Epoch 11: train loss: 1.0114154815673828; test loss: 1.0363315343856812\n",
      "Epoch 11: train accuracy: 0.7336895388076491; test accuracy: 0.7210348706411699\n",
      "12 0\n",
      "12 200\n",
      "12 400\n",
      "12 600\n",
      "12 800\n",
      "12 1000\n",
      "12 1200\n",
      "12 1400\n",
      "12 1600\n",
      "12 1800\n",
      "12 2000\n",
      "12 2200\n",
      "12 2400\n",
      "12 2600\n",
      "12 2800\n",
      "12 3000\n",
      "12 3200\n",
      "12 3400\n",
      "Epoch 12: train loss: 1.0080486536026; test loss: 1.0232043266296387\n",
      "Epoch 12: train accuracy: 0.7410011248593926; test accuracy: 0.7255343082114736\n",
      "13 0\n",
      "13 200\n",
      "13 400\n",
      "13 600\n",
      "13 800\n",
      "13 1000\n",
      "13 1200\n",
      "13 1400\n",
      "13 1600\n",
      "13 1800\n",
      "13 2000\n",
      "13 2200\n",
      "13 2400\n",
      "13 2600\n",
      "13 2800\n",
      "13 3000\n",
      "13 3200\n",
      "13 3400\n",
      "Epoch 13: train loss: 1.0062013864517212; test loss: 1.0138475894927979\n",
      "Epoch 13: train accuracy: 0.749437570303712; test accuracy: 0.7311586051743532\n",
      "14 0\n",
      "14 200\n",
      "14 400\n",
      "14 600\n",
      "14 800\n",
      "14 1000\n",
      "14 1200\n",
      "14 1400\n",
      "14 1600\n",
      "14 1800\n",
      "14 2000\n",
      "14 2200\n",
      "14 2400\n",
      "14 2600\n",
      "14 2800\n",
      "14 3000\n",
      "14 3200\n",
      "14 3400\n",
      "Epoch 14: train loss: 1.0156230926513672; test loss: 1.0201915502548218\n",
      "Epoch 14: train accuracy: 0.7530933633295838; test accuracy: 0.7289088863892014\n",
      "15 0\n",
      "15 200\n",
      "15 400\n",
      "15 600\n",
      "15 800\n",
      "15 1000\n",
      "15 1200\n",
      "15 1400\n",
      "15 1600\n",
      "15 1800\n",
      "15 2000\n",
      "15 2200\n",
      "15 2400\n",
      "15 2600\n",
      "15 2800\n",
      "15 3000\n",
      "15 3200\n",
      "15 3400\n",
      "Epoch 15: train loss: 0.9985557198524475; test loss: 1.0105290412902832\n",
      "Epoch 15: train accuracy: 0.7550618672665916; test accuracy: 0.7311586051743532\n",
      "16 0\n",
      "16 200\n",
      "16 400\n",
      "16 600\n",
      "16 800\n",
      "16 1000\n",
      "16 1200\n",
      "16 1400\n",
      "16 1600\n",
      "16 1800\n",
      "16 2000\n",
      "16 2200\n",
      "16 2400\n",
      "16 2600\n",
      "16 2800\n",
      "16 3000\n",
      "16 3200\n",
      "16 3400\n",
      "Epoch 16: train loss: 0.9984285235404968; test loss: 1.0053470134735107\n",
      "Epoch 16: train accuracy: 0.7649043869516311; test accuracy: 0.7412823397075365\n",
      "17 0\n",
      "17 200\n",
      "17 400\n",
      "17 600\n",
      "17 800\n",
      "17 1000\n",
      "17 1200\n",
      "17 1400\n",
      "17 1600\n",
      "17 1800\n",
      "17 2000\n",
      "17 2200\n",
      "17 2400\n",
      "17 2600\n",
      "17 2800\n",
      "17 3000\n",
      "17 3200\n",
      "17 3400\n",
      "Epoch 17: train loss: 0.9934443235397339; test loss: 1.0003832578659058\n",
      "Epoch 17: train accuracy: 0.7705286839145107; test accuracy: 0.7412823397075365\n",
      "18 0\n",
      "18 200\n",
      "18 400\n",
      "18 600\n",
      "18 800\n",
      "18 1000\n",
      "18 1200\n",
      "18 1400\n",
      "18 1600\n",
      "18 1800\n",
      "18 2000\n",
      "18 2200\n",
      "18 2400\n",
      "18 2600\n",
      "18 2800\n",
      "18 3000\n",
      "18 3200\n",
      "18 3400\n",
      "Epoch 18: train loss: 0.9866841435432434; test loss: 0.996870219707489\n",
      "Epoch 18: train accuracy: 0.764341957255343; test accuracy: 0.7435320584926884\n",
      "19 0\n",
      "19 200\n",
      "19 400\n",
      "19 600\n",
      "19 800\n",
      "19 1000\n",
      "19 1200\n",
      "19 1400\n",
      "19 1600\n",
      "19 1800\n",
      "19 2000\n",
      "19 2200\n",
      "19 2400\n",
      "19 2600\n",
      "19 2800\n",
      "19 3000\n",
      "19 3200\n",
      "19 3400\n",
      "Epoch 19: train loss: 0.9852752685546875; test loss: 0.9937304258346558\n",
      "Epoch 19: train accuracy: 0.7764341957255343; test accuracy: 0.7547806524184477\n",
      "20 0\n",
      "20 200\n",
      "20 400\n",
      "20 600\n",
      "20 800\n",
      "20 1000\n",
      "20 1200\n",
      "20 1400\n",
      "20 1600\n",
      "20 1800\n",
      "20 2000\n",
      "20 2200\n",
      "20 2400\n",
      "20 2600\n",
      "20 2800\n",
      "20 3000\n",
      "20 3200\n",
      "20 3400\n",
      "Epoch 20: train loss: 0.9833945035934448; test loss: 0.9898610711097717\n",
      "Epoch 20: train accuracy: 0.78177727784027; test accuracy: 0.7570303712035995\n",
      "21 0\n",
      "21 200\n",
      "21 400\n",
      "21 600\n",
      "21 800\n",
      "21 1000\n",
      "21 1200\n",
      "21 1400\n",
      "21 1600\n",
      "21 1800\n",
      "21 2000\n",
      "21 2200\n",
      "21 2400\n",
      "21 2600\n",
      "21 2800\n",
      "21 3000\n",
      "21 3200\n",
      "21 3400\n",
      "Epoch 21: train loss: 0.9800364971160889; test loss: 0.987986147403717\n",
      "Epoch 21: train accuracy: 0.78037120359955; test accuracy: 0.7581552305961755\n",
      "22 0\n",
      "22 200\n",
      "22 400\n",
      "22 600\n",
      "22 800\n",
      "22 1000\n",
      "22 1200\n",
      "22 1400\n",
      "22 1600\n",
      "22 1800\n",
      "22 2000\n",
      "22 2200\n",
      "22 2400\n",
      "22 2600\n",
      "22 2800\n",
      "22 3000\n",
      "22 3200\n",
      "22 3400\n",
      "Epoch 22: train loss: 0.9817704558372498; test loss: 0.9868204593658447\n",
      "Epoch 22: train accuracy: 0.7882452193475815; test accuracy: 0.7626546681664792\n",
      "23 0\n",
      "23 200\n",
      "23 400\n",
      "23 600\n",
      "23 800\n",
      "23 1000\n",
      "23 1200\n",
      "23 1400\n",
      "23 1600\n",
      "23 1800\n",
      "23 2000\n",
      "23 2200\n",
      "23 2400\n",
      "23 2600\n",
      "23 2800\n",
      "23 3000\n",
      "23 3200\n",
      "23 3400\n",
      "Epoch 23: train loss: 0.9783046245574951; test loss: 0.988213300704956\n",
      "Epoch 23: train accuracy: 0.7845894263217098; test accuracy: 0.7570303712035995\n",
      "24 0\n",
      "24 200\n",
      "24 400\n",
      "24 600\n",
      "24 800\n",
      "24 1000\n",
      "24 1200\n",
      "24 1400\n",
      "24 1600\n",
      "24 1800\n",
      "24 2000\n",
      "24 2200\n",
      "24 2400\n",
      "24 2600\n",
      "24 2800\n",
      "24 3000\n",
      "24 3200\n",
      "24 3400\n",
      "Epoch 24: train loss: 0.9734959602355957; test loss: 0.9811897873878479\n",
      "Epoch 24: train accuracy: 0.7919010123734533; test accuracy: 0.7671541057367829\n",
      "25 0\n",
      "25 200\n",
      "25 400\n",
      "25 600\n",
      "25 800\n",
      "25 1000\n",
      "25 1200\n",
      "25 1400\n",
      "25 1600\n",
      "25 1800\n",
      "25 2000\n",
      "25 2200\n",
      "25 2400\n",
      "25 2600\n",
      "25 2800\n",
      "25 3000\n",
      "25 3200\n",
      "25 3400\n",
      "Epoch 25: train loss: 0.97503262758255; test loss: 0.9809976816177368\n",
      "Epoch 25: train accuracy: 0.7935883014623172; test accuracy: 0.766029246344207\n",
      "26 0\n",
      "26 200\n",
      "26 400\n",
      "26 600\n",
      "26 800\n",
      "26 1000\n",
      "26 1200\n",
      "26 1400\n",
      "26 1600\n",
      "26 1800\n",
      "26 2000\n",
      "26 2200\n",
      "26 2400\n",
      "26 2600\n",
      "26 2800\n",
      "26 3000\n",
      "26 3200\n",
      "26 3400\n",
      "Epoch 26: train loss: 0.9693894982337952; test loss: 0.9776543378829956\n",
      "Epoch 26: train accuracy: 0.7980877390326209; test accuracy: 0.7705286839145107\n",
      "27 0\n",
      "27 200\n",
      "27 400\n",
      "27 600\n",
      "27 800\n",
      "27 1000\n",
      "27 1200\n",
      "27 1400\n",
      "27 1600\n",
      "27 1800\n",
      "27 2000\n",
      "27 2200\n",
      "27 2400\n",
      "27 2600\n",
      "27 2800\n",
      "27 3000\n",
      "27 3200\n",
      "27 3400\n",
      "Epoch 27: train loss: 0.9710806012153625; test loss: 0.9765707850456238\n",
      "Epoch 27: train accuracy: 0.7986501687289089; test accuracy: 0.7694038245219348\n",
      "28 0\n",
      "28 200\n",
      "28 400\n",
      "28 600\n",
      "28 800\n",
      "28 1000\n",
      "28 1200\n",
      "28 1400\n",
      "28 1600\n",
      "28 1800\n",
      "28 2000\n",
      "28 2200\n",
      "28 2400\n",
      "28 2600\n",
      "28 2800\n",
      "28 3000\n",
      "28 3200\n",
      "28 3400\n",
      "Epoch 28: train loss: 0.9671714901924133; test loss: 0.9748061299324036\n",
      "Epoch 28: train accuracy: 0.7992125984251969; test accuracy: 0.7716535433070866\n",
      "29 0\n",
      "29 200\n",
      "29 400\n",
      "29 600\n",
      "29 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29 1000\n",
      "29 1200\n",
      "29 1400\n",
      "29 1600\n",
      "29 1800\n",
      "29 2000\n",
      "29 2200\n",
      "29 2400\n",
      "29 2600\n",
      "29 2800\n",
      "29 3000\n",
      "29 3200\n",
      "29 3400\n",
      "Epoch 29: train loss: 0.9716664552688599; test loss: 0.9733487367630005\n",
      "Epoch 29: train accuracy: 0.797806524184477; test accuracy: 0.7716535433070866\n",
      "30 0\n",
      "30 200\n",
      "30 400\n",
      "30 600\n",
      "30 800\n",
      "30 1000\n",
      "30 1200\n",
      "30 1400\n",
      "30 1600\n",
      "30 1800\n",
      "30 2000\n",
      "30 2200\n",
      "30 2400\n",
      "30 2600\n",
      "30 2800\n",
      "30 3000\n",
      "30 3200\n",
      "30 3400\n",
      "Epoch 30: train loss: 0.969884991645813; test loss: 0.9737409353256226\n",
      "Epoch 30: train accuracy: 0.8000562429696289; test accuracy: 0.7727784026996626\n",
      "31 0\n",
      "31 200\n",
      "31 400\n",
      "31 600\n",
      "31 800\n",
      "31 1000\n",
      "31 1200\n",
      "31 1400\n",
      "31 1600\n",
      "31 1800\n",
      "31 2000\n",
      "31 2200\n",
      "31 2400\n",
      "31 2600\n",
      "31 2800\n",
      "31 3000\n",
      "31 3200\n",
      "31 3400\n",
      "Epoch 31: train loss: 0.9597621560096741; test loss: 0.9665184020996094\n",
      "Epoch 31: train accuracy: 0.8008998875140607; test accuracy: 0.7750281214848144\n",
      "32 0\n",
      "32 200\n",
      "32 400\n",
      "32 600\n",
      "32 800\n",
      "32 1000\n",
      "32 1200\n",
      "32 1400\n",
      "32 1600\n",
      "32 1800\n",
      "32 2000\n",
      "32 2200\n",
      "32 2400\n",
      "32 2600\n",
      "32 2800\n",
      "32 3000\n",
      "32 3200\n",
      "32 3400\n",
      "Epoch 32: train loss: 0.9673858880996704; test loss: 0.9681379795074463\n",
      "Epoch 32: train accuracy: 0.8025871766029247; test accuracy: 0.7761529808773904\n",
      "33 0\n",
      "33 200\n",
      "33 400\n",
      "33 600\n",
      "33 800\n",
      "33 1000\n",
      "33 1200\n",
      "33 1400\n",
      "33 1600\n",
      "33 1800\n",
      "33 2000\n",
      "33 2200\n",
      "33 2400\n",
      "33 2600\n",
      "33 2800\n",
      "33 3000\n",
      "33 3200\n",
      "33 3400\n",
      "Epoch 33: train loss: 0.959758460521698; test loss: 0.9667963981628418\n",
      "Epoch 33: train accuracy: 0.8003374578177728; test accuracy: 0.7761529808773904\n",
      "34 0\n",
      "34 200\n",
      "34 400\n",
      "34 600\n",
      "34 800\n",
      "34 1000\n",
      "34 1200\n",
      "34 1400\n",
      "34 1600\n",
      "34 1800\n",
      "34 2000\n",
      "34 2200\n",
      "34 2400\n",
      "34 2600\n",
      "34 2800\n",
      "34 3000\n",
      "34 3200\n",
      "34 3400\n",
      "Epoch 34: train loss: 0.9642919898033142; test loss: 0.9671265482902527\n",
      "Epoch 34: train accuracy: 0.8017435320584927; test accuracy: 0.7750281214848144\n",
      "35 0\n",
      "35 200\n",
      "35 400\n",
      "35 600\n",
      "35 800\n",
      "35 1000\n",
      "35 1200\n",
      "35 1400\n",
      "35 1600\n",
      "35 1800\n",
      "35 2000\n",
      "35 2200\n",
      "35 2400\n",
      "35 2600\n",
      "35 2800\n",
      "35 3000\n",
      "35 3200\n",
      "35 3400\n",
      "Epoch 35: train loss: 0.9615095853805542; test loss: 0.9648520350456238\n",
      "Epoch 35: train accuracy: 0.8039932508436446; test accuracy: 0.7795275590551181\n",
      "36 0\n",
      "36 200\n",
      "36 400\n",
      "36 600\n",
      "36 800\n",
      "36 1000\n",
      "36 1200\n",
      "36 1400\n",
      "36 1600\n",
      "36 1800\n",
      "36 2000\n",
      "36 2200\n",
      "36 2400\n",
      "36 2600\n",
      "36 2800\n",
      "36 3000\n",
      "36 3200\n",
      "36 3400\n",
      "Epoch 36: train loss: 0.9610910415649414; test loss: 0.9630519151687622\n",
      "Epoch 36: train accuracy: 0.8025871766029247; test accuracy: 0.7784026996625422\n",
      "37 0\n",
      "37 200\n",
      "37 400\n",
      "37 600\n",
      "37 800\n",
      "37 1000\n",
      "37 1200\n",
      "37 1400\n",
      "37 1600\n",
      "37 1800\n",
      "37 2000\n",
      "37 2200\n",
      "37 2400\n",
      "37 2600\n",
      "37 2800\n",
      "37 3000\n",
      "37 3200\n",
      "37 3400\n",
      "Epoch 37: train loss: 0.957322359085083; test loss: 0.9659034609794617\n",
      "Epoch 37: train accuracy: 0.8017435320584927; test accuracy: 0.7761529808773904\n",
      "38 0\n",
      "38 200\n",
      "38 400\n",
      "38 600\n",
      "38 800\n",
      "38 1000\n",
      "38 1200\n",
      "38 1400\n",
      "38 1600\n",
      "38 1800\n",
      "38 2000\n",
      "38 2200\n",
      "38 2400\n",
      "38 2600\n",
      "38 2800\n",
      "38 3000\n",
      "38 3200\n",
      "38 3400\n",
      "Epoch 38: train loss: 0.9622710943222046; test loss: 0.9637900590896606\n",
      "Epoch 38: train accuracy: 0.8028683914510686; test accuracy: 0.7784026996625422\n",
      "39 0\n",
      "39 200\n",
      "39 400\n",
      "39 600\n",
      "39 800\n",
      "39 1000\n",
      "39 1200\n",
      "39 1400\n",
      "39 1600\n",
      "39 1800\n",
      "39 2000\n",
      "39 2200\n",
      "39 2400\n",
      "39 2600\n",
      "39 2800\n",
      "39 3000\n",
      "39 3200\n",
      "39 3400\n",
      "Epoch 39: train loss: 0.9555616974830627; test loss: 0.9644184112548828\n",
      "Epoch 39: train accuracy: 0.8051181102362205; test accuracy: 0.78177727784027\n",
      "40 0\n",
      "40 200\n",
      "40 400\n",
      "40 600\n",
      "40 800\n",
      "40 1000\n",
      "40 1200\n",
      "40 1400\n",
      "40 1600\n",
      "40 1800\n",
      "40 2000\n",
      "40 2200\n",
      "40 2400\n",
      "40 2600\n",
      "40 2800\n",
      "40 3000\n",
      "40 3200\n",
      "40 3400\n",
      "Epoch 40: train loss: 0.9602763652801514; test loss: 0.9613738656044006\n",
      "Epoch 40: train accuracy: 0.8037120359955006; test accuracy: 0.7806524184476941\n",
      "41 0\n",
      "41 200\n",
      "41 400\n",
      "41 600\n",
      "41 800\n",
      "41 1000\n",
      "41 1200\n",
      "41 1400\n",
      "41 1600\n",
      "41 1800\n",
      "41 2000\n",
      "41 2200\n",
      "41 2400\n",
      "41 2600\n",
      "41 2800\n",
      "41 3000\n",
      "41 3200\n",
      "41 3400\n",
      "Epoch 41: train loss: 0.9560214281082153; test loss: 0.9642133116722107\n",
      "Epoch 41: train accuracy: 0.8028683914510686; test accuracy: 0.7784026996625422\n",
      "42 0\n",
      "42 200\n",
      "42 400\n",
      "42 600\n",
      "42 800\n",
      "42 1000\n",
      "42 1200\n",
      "42 1400\n",
      "42 1600\n",
      "42 1800\n",
      "42 2000\n",
      "42 2200\n",
      "42 2400\n",
      "42 2600\n",
      "42 2800\n",
      "42 3000\n",
      "42 3200\n",
      "42 3400\n",
      "Epoch 42: train loss: 0.9605075120925903; test loss: 0.962281346321106\n",
      "Epoch 42: train accuracy: 0.8023059617547806; test accuracy: 0.7784026996625422\n",
      "43 0\n",
      "43 200\n",
      "43 400\n",
      "43 600\n",
      "43 800\n",
      "43 1000\n",
      "43 1200\n",
      "43 1400\n",
      "43 1600\n",
      "43 1800\n",
      "43 2000\n",
      "43 2200\n",
      "43 2400\n",
      "43 2600\n",
      "43 2800\n",
      "43 3000\n",
      "43 3200\n",
      "43 3400\n",
      "Epoch 43: train loss: 0.9555473327636719; test loss: 0.9614682197570801\n",
      "Epoch 43: train accuracy: 0.8068053993250843; test accuracy: 0.78177727784027\n",
      "44 0\n",
      "44 200\n",
      "44 400\n",
      "44 600\n",
      "44 800\n",
      "44 1000\n",
      "44 1200\n",
      "44 1400\n",
      "44 1600\n",
      "44 1800\n",
      "44 2000\n",
      "44 2200\n",
      "44 2400\n",
      "44 2600\n",
      "44 2800\n",
      "44 3000\n",
      "44 3200\n",
      "44 3400\n",
      "Epoch 44: train loss: 0.9589439630508423; test loss: 0.9588154554367065\n",
      "Epoch 44: train accuracy: 0.8068053993250843; test accuracy: 0.7851518560179978\n",
      "45 0\n",
      "45 200\n",
      "45 400\n",
      "45 600\n",
      "45 800\n",
      "45 1000\n",
      "45 1200\n",
      "45 1400\n",
      "45 1600\n",
      "45 1800\n",
      "45 2000\n",
      "45 2200\n",
      "45 2400\n",
      "45 2600\n",
      "45 2800\n",
      "45 3000\n",
      "45 3200\n",
      "45 3400\n",
      "Epoch 45: train loss: 0.9505254030227661; test loss: 0.9562164545059204\n",
      "Epoch 45: train accuracy: 0.8110236220472441; test accuracy: 0.7885264341957255\n",
      "46 0\n",
      "46 200\n",
      "46 400\n",
      "46 600\n",
      "46 800\n",
      "46 1000\n",
      "46 1200\n",
      "46 1400\n",
      "46 1600\n",
      "46 1800\n",
      "46 2000\n",
      "46 2200\n",
      "46 2400\n",
      "46 2600\n",
      "46 2800\n",
      "46 3000\n",
      "46 3200\n",
      "46 3400\n",
      "Epoch 46: train loss: 0.95172518491745; test loss: 0.943107545375824\n",
      "Epoch 46: train accuracy: 0.811586051743532; test accuracy: 0.8008998875140607\n",
      "47 0\n",
      "47 200\n",
      "47 400\n",
      "47 600\n",
      "47 800\n",
      "47 1000\n",
      "47 1200\n",
      "47 1400\n",
      "47 1600\n",
      "47 1800\n",
      "47 2000\n",
      "47 2200\n",
      "47 2400\n",
      "47 2600\n",
      "47 2800\n",
      "47 3000\n",
      "47 3200\n",
      "47 3400\n",
      "Epoch 47: train loss: 0.9381769895553589; test loss: 0.9421346783638\n",
      "Epoch 47: train accuracy: 0.8143982002249719; test accuracy: 0.8020247469066367\n",
      "48 0\n",
      "48 200\n",
      "48 400\n",
      "48 600\n",
      "48 800\n",
      "48 1000\n",
      "48 1200\n",
      "48 1400\n",
      "48 1600\n",
      "48 1800\n",
      "48 2000\n",
      "48 2200\n",
      "48 2400\n",
      "48 2600\n",
      "48 2800\n",
      "48 3000\n",
      "48 3200\n",
      "48 3400\n",
      "Epoch 48: train loss: 0.9356233477592468; test loss: 0.9399043321609497\n",
      "Epoch 48: train accuracy: 0.8174915635545557; test accuracy: 0.8065241844769404\n",
      "49 0\n",
      "49 200\n",
      "49 400\n",
      "49 600\n",
      "49 800\n",
      "49 1000\n",
      "49 1200\n",
      "49 1400\n",
      "49 1600\n",
      "49 1800\n",
      "49 2000\n",
      "49 2200\n",
      "49 2400\n",
      "49 2600\n",
      "49 2800\n",
      "49 3000\n",
      "49 3200\n",
      "49 3400\n",
      "Epoch 49: train loss: 0.9443773031234741; test loss: 0.9423878788948059\n",
      "Epoch 49: train accuracy: 0.812429696287964; test accuracy: 0.7997750281214848\n",
      "50 0\n",
      "50 200\n",
      "50 400\n",
      "50 600\n",
      "50 800\n",
      "50 1000\n",
      "50 1200\n",
      "50 1400\n",
      "50 1600\n",
      "50 1800\n",
      "50 2000\n",
      "50 2200\n",
      "50 2400\n",
      "50 2600\n",
      "50 2800\n",
      "50 3000\n",
      "50 3200\n",
      "50 3400\n",
      "Epoch 50: train loss: 0.9434457421302795; test loss: 0.9415180087089539\n",
      "Epoch 50: train accuracy: 0.8191788526434196; test accuracy: 0.8053993250843644\n",
      "51 0\n",
      "51 200\n",
      "51 400\n",
      "51 600\n",
      "51 800\n",
      "51 1000\n",
      "51 1200\n",
      "51 1400\n",
      "51 1600\n",
      "51 1800\n",
      "51 2000\n",
      "51 2200\n",
      "51 2400\n",
      "51 2600\n",
      "51 2800\n",
      "51 3000\n",
      "51 3200\n",
      "51 3400\n",
      "Epoch 51: train loss: 0.9409988522529602; test loss: 0.9402338266372681\n",
      "Epoch 51: train accuracy: 0.8149606299212598; test accuracy: 0.8020247469066367\n",
      "52 0\n",
      "52 200\n",
      "52 400\n",
      "52 600\n",
      "52 800\n",
      "52 1000\n",
      "52 1200\n",
      "52 1400\n",
      "52 1600\n",
      "52 1800\n",
      "52 2000\n",
      "52 2200\n",
      "52 2400\n",
      "52 2600\n",
      "52 2800\n",
      "52 3000\n",
      "52 3200\n",
      "52 3400\n",
      "Epoch 52: train loss: 0.9430555701255798; test loss: 0.9403904676437378\n",
      "Epoch 52: train accuracy: 0.8169291338582677; test accuracy: 0.8042744656917885\n",
      "53 0\n",
      "53 200\n",
      "53 400\n",
      "53 600\n",
      "53 800\n",
      "53 1000\n",
      "53 1200\n",
      "53 1400\n",
      "53 1600\n",
      "53 1800\n",
      "53 2000\n",
      "53 2200\n",
      "53 2400\n",
      "53 2600\n",
      "53 2800\n",
      "53 3000\n",
      "53 3200\n",
      "53 3400\n",
      "Epoch 53: train loss: 0.9299560785293579; test loss: 0.9416907429695129\n",
      "Epoch 53: train accuracy: 0.8158042744656918; test accuracy: 0.8020247469066367\n",
      "54 0\n",
      "54 200\n",
      "54 400\n",
      "54 600\n",
      "54 800\n",
      "54 1000\n",
      "54 1200\n",
      "54 1400\n",
      "54 1600\n",
      "54 1800\n",
      "54 2000\n",
      "54 2200\n",
      "54 2400\n",
      "54 2600\n",
      "54 2800\n",
      "54 3000\n",
      "54 3200\n",
      "54 3400\n",
      "Epoch 54: train loss: 0.9223721027374268; test loss: 0.9384291172027588\n",
      "Epoch 54: train accuracy: 0.8200224971878515; test accuracy: 0.8042744656917885\n",
      "55 0\n",
      "55 200\n",
      "55 400\n",
      "55 600\n",
      "55 800\n",
      "55 1000\n",
      "55 1200\n",
      "55 1400\n",
      "55 1600\n",
      "55 1800\n",
      "55 2000\n",
      "55 2200\n",
      "55 2400\n",
      "55 2600\n",
      "55 2800\n",
      "55 3000\n",
      "55 3200\n",
      "55 3400\n",
      "Epoch 55: train loss: 0.9196130633354187; test loss: 0.937568187713623\n",
      "Epoch 55: train accuracy: 0.8214285714285714; test accuracy: 0.8053993250843644\n",
      "56 0\n",
      "56 200\n",
      "56 400\n",
      "56 600\n",
      "56 800\n",
      "56 1000\n",
      "56 1200\n",
      "56 1400\n",
      "56 1600\n",
      "56 1800\n",
      "56 2000\n",
      "56 2200\n",
      "56 2400\n",
      "56 2600\n",
      "56 2800\n",
      "56 3000\n",
      "56 3200\n",
      "56 3400\n",
      "Epoch 56: train loss: 0.9200430512428284; test loss: 0.9413301944732666\n",
      "Epoch 56: train accuracy: 0.8197412823397076; test accuracy: 0.8008998875140607\n",
      "57 0\n",
      "57 200\n",
      "57 400\n",
      "57 600\n",
      "57 800\n",
      "57 1000\n",
      "57 1200\n",
      "57 1400\n",
      "57 1600\n",
      "57 1800\n",
      "57 2000\n",
      "57 2200\n",
      "57 2400\n",
      "57 2600\n",
      "57 2800\n",
      "57 3000\n",
      "57 3200\n",
      "57 3400\n",
      "Epoch 57: train loss: 0.922438383102417; test loss: 0.9375696182250977\n",
      "Epoch 57: train accuracy: 0.8236782902137233; test accuracy: 0.8087739032620922\n",
      "58 0\n",
      "58 200\n",
      "58 400\n",
      "58 600\n",
      "58 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 1000\n",
      "58 1200\n",
      "58 1400\n",
      "58 1600\n",
      "58 1800\n",
      "58 2000\n",
      "58 2200\n",
      "58 2400\n",
      "58 2600\n",
      "58 2800\n",
      "58 3000\n",
      "58 3200\n",
      "58 3400\n",
      "Epoch 58: train loss: 0.9234593510627747; test loss: 0.9369968175888062\n",
      "Epoch 58: train accuracy: 0.8219910011248593; test accuracy: 0.8042744656917885\n",
      "59 0\n",
      "59 200\n",
      "59 400\n",
      "59 600\n",
      "59 800\n",
      "59 1000\n",
      "59 1200\n",
      "59 1400\n",
      "59 1600\n",
      "59 1800\n",
      "59 2000\n",
      "59 2200\n",
      "59 2400\n",
      "59 2600\n",
      "59 2800\n",
      "59 3000\n",
      "59 3200\n",
      "59 3400\n",
      "Epoch 59: train loss: 0.918164849281311; test loss: 0.9362740516662598\n",
      "Epoch 59: train accuracy: 0.8264904386951631; test accuracy: 0.8098987626546682\n",
      "60 0\n",
      "60 200\n",
      "60 400\n",
      "60 600\n",
      "60 800\n",
      "60 1000\n",
      "60 1200\n",
      "60 1400\n",
      "60 1600\n",
      "60 1800\n",
      "60 2000\n",
      "60 2200\n",
      "60 2400\n",
      "60 2600\n",
      "60 2800\n",
      "60 3000\n",
      "60 3200\n",
      "60 3400\n",
      "Epoch 60: train loss: 0.9138844013214111; test loss: 0.9355329871177673\n",
      "Epoch 60: train accuracy: 0.8205849268841395; test accuracy: 0.8053993250843644\n",
      "61 0\n",
      "61 200\n",
      "61 400\n",
      "61 600\n",
      "61 800\n",
      "61 1000\n",
      "61 1200\n",
      "61 1400\n",
      "61 1600\n",
      "61 1800\n",
      "61 2000\n",
      "61 2200\n",
      "61 2400\n",
      "61 2600\n",
      "61 2800\n",
      "61 3000\n",
      "61 3200\n",
      "61 3400\n",
      "Epoch 61: train loss: 0.9169493317604065; test loss: 0.9341381788253784\n",
      "Epoch 61: train accuracy: 0.8248031496062992; test accuracy: 0.8098987626546682\n",
      "62 0\n",
      "62 200\n",
      "62 400\n",
      "62 600\n",
      "62 800\n",
      "62 1000\n",
      "62 1200\n",
      "62 1400\n",
      "62 1600\n",
      "62 1800\n",
      "62 2000\n",
      "62 2200\n",
      "62 2400\n",
      "62 2600\n",
      "62 2800\n",
      "62 3000\n",
      "62 3200\n",
      "62 3400\n",
      "Epoch 62: train loss: 0.9196025133132935; test loss: 0.9367838501930237\n",
      "Epoch 62: train accuracy: 0.8231158605174353; test accuracy: 0.8065241844769404\n",
      "63 0\n",
      "63 200\n",
      "63 400\n",
      "63 600\n",
      "63 800\n",
      "63 1000\n",
      "63 1200\n",
      "63 1400\n",
      "63 1600\n",
      "63 1800\n",
      "63 2000\n",
      "63 2200\n",
      "63 2400\n",
      "63 2600\n",
      "63 2800\n",
      "63 3000\n",
      "63 3200\n",
      "63 3400\n",
      "Epoch 63: train loss: 0.9148339629173279; test loss: 0.937411904335022\n",
      "Epoch 63: train accuracy: 0.8231158605174353; test accuracy: 0.8042744656917885\n",
      "64 0\n",
      "64 200\n",
      "64 400\n",
      "64 600\n",
      "64 800\n",
      "64 1000\n",
      "64 1200\n",
      "64 1400\n",
      "64 1600\n",
      "64 1800\n",
      "64 2000\n",
      "64 2200\n",
      "64 2400\n",
      "64 2600\n",
      "64 2800\n",
      "64 3000\n",
      "64 3200\n",
      "64 3400\n",
      "Epoch 64: train loss: 0.9107182621955872; test loss: 0.9361847639083862\n",
      "Epoch 64: train accuracy: 0.8245219347581553; test accuracy: 0.8042744656917885\n",
      "65 0\n",
      "65 200\n",
      "65 400\n",
      "65 600\n",
      "65 800\n",
      "65 1000\n",
      "65 1200\n",
      "65 1400\n",
      "65 1600\n",
      "65 1800\n",
      "65 2000\n",
      "65 2200\n",
      "65 2400\n",
      "65 2600\n",
      "65 2800\n",
      "65 3000\n",
      "65 3200\n",
      "65 3400\n",
      "Epoch 65: train loss: 0.9105045795440674; test loss: 0.934847891330719\n",
      "Epoch 65: train accuracy: 0.827334083239595; test accuracy: 0.8076490438695163\n",
      "66 0\n",
      "66 200\n",
      "66 400\n",
      "66 600\n",
      "66 800\n",
      "66 1000\n",
      "66 1200\n",
      "66 1400\n",
      "66 1600\n",
      "66 1800\n",
      "66 2000\n",
      "66 2200\n",
      "66 2400\n",
      "66 2600\n",
      "66 2800\n",
      "66 3000\n",
      "66 3200\n",
      "66 3400\n",
      "Epoch 66: train loss: 0.9095773100852966; test loss: 0.9345894455909729\n",
      "Epoch 66: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "67 0\n",
      "67 200\n",
      "67 400\n",
      "67 600\n",
      "67 800\n",
      "67 1000\n",
      "67 1200\n",
      "67 1400\n",
      "67 1600\n",
      "67 1800\n",
      "67 2000\n",
      "67 2200\n",
      "67 2400\n",
      "67 2600\n",
      "67 2800\n",
      "67 3000\n",
      "67 3200\n",
      "67 3400\n",
      "Epoch 67: train loss: 0.9093441367149353; test loss: 0.9348482489585876\n",
      "Epoch 67: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "68 0\n",
      "68 200\n",
      "68 400\n",
      "68 600\n",
      "68 800\n",
      "68 1000\n",
      "68 1200\n",
      "68 1400\n",
      "68 1600\n",
      "68 1800\n",
      "68 2000\n",
      "68 2200\n",
      "68 2400\n",
      "68 2600\n",
      "68 2800\n",
      "68 3000\n",
      "68 3200\n",
      "68 3400\n",
      "Epoch 68: train loss: 0.9087425470352173; test loss: 0.9342727661132812\n",
      "Epoch 68: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "69 0\n",
      "69 200\n",
      "69 400\n",
      "69 600\n",
      "69 800\n",
      "69 1000\n",
      "69 1200\n",
      "69 1400\n",
      "69 1600\n",
      "69 1800\n",
      "69 2000\n",
      "69 2200\n",
      "69 2400\n",
      "69 2600\n",
      "69 2800\n",
      "69 3000\n",
      "69 3200\n",
      "69 3400\n",
      "Epoch 69: train loss: 0.9085978865623474; test loss: 0.9342111945152283\n",
      "Epoch 69: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "70 0\n",
      "70 200\n",
      "70 400\n",
      "70 600\n",
      "70 800\n",
      "70 1000\n",
      "70 1200\n",
      "70 1400\n",
      "70 1600\n",
      "70 1800\n",
      "70 2000\n",
      "70 2200\n",
      "70 2400\n",
      "70 2600\n",
      "70 2800\n",
      "70 3000\n",
      "70 3200\n",
      "70 3400\n",
      "Epoch 70: train loss: 0.9089676141738892; test loss: 0.9349797964096069\n",
      "Epoch 70: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "71 0\n",
      "71 200\n",
      "71 400\n",
      "71 600\n",
      "71 800\n",
      "71 1000\n",
      "71 1200\n",
      "71 1400\n",
      "71 1600\n",
      "71 1800\n",
      "71 2000\n",
      "71 2200\n",
      "71 2400\n",
      "71 2600\n",
      "71 2800\n",
      "71 3000\n",
      "71 3200\n",
      "71 3400\n",
      "Epoch 71: train loss: 0.9083985686302185; test loss: 0.9342588782310486\n",
      "Epoch 71: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "72 0\n",
      "72 200\n",
      "72 400\n",
      "72 600\n",
      "72 800\n",
      "72 1000\n",
      "72 1200\n",
      "72 1400\n",
      "72 1600\n",
      "72 1800\n",
      "72 2000\n",
      "72 2200\n",
      "72 2400\n",
      "72 2600\n",
      "72 2800\n",
      "72 3000\n",
      "72 3200\n",
      "72 3400\n",
      "Epoch 72: train loss: 0.9084873795509338; test loss: 0.9345940947532654\n",
      "Epoch 72: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "73 0\n",
      "73 200\n",
      "73 400\n",
      "73 600\n",
      "73 800\n",
      "73 1000\n",
      "73 1200\n",
      "73 1400\n",
      "73 1600\n",
      "73 1800\n",
      "73 2000\n",
      "73 2200\n",
      "73 2400\n",
      "73 2600\n",
      "73 2800\n",
      "73 3000\n",
      "73 3200\n",
      "73 3400\n",
      "Epoch 73: train loss: 0.9082058668136597; test loss: 0.9341102242469788\n",
      "Epoch 73: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "74 0\n",
      "74 200\n",
      "74 400\n",
      "74 600\n",
      "74 800\n",
      "74 1000\n",
      "74 1200\n",
      "74 1400\n",
      "74 1600\n",
      "74 1800\n",
      "74 2000\n",
      "74 2200\n",
      "74 2400\n",
      "74 2600\n",
      "74 2800\n",
      "74 3000\n",
      "74 3200\n",
      "74 3400\n",
      "Epoch 74: train loss: 0.9075556993484497; test loss: 0.9338201284408569\n",
      "Epoch 74: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "75 0\n",
      "75 200\n",
      "75 400\n",
      "75 600\n",
      "75 800\n",
      "75 1000\n",
      "75 1200\n",
      "75 1400\n",
      "75 1600\n",
      "75 1800\n",
      "75 2000\n",
      "75 2200\n",
      "75 2400\n",
      "75 2600\n",
      "75 2800\n",
      "75 3000\n",
      "75 3200\n",
      "75 3400\n",
      "Epoch 75: train loss: 0.9089787006378174; test loss: 0.9349677562713623\n",
      "Epoch 75: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "76 0\n",
      "76 200\n",
      "76 400\n",
      "76 600\n",
      "76 800\n",
      "76 1000\n",
      "76 1200\n",
      "76 1400\n",
      "76 1600\n",
      "76 1800\n",
      "76 2000\n",
      "76 2200\n",
      "76 2400\n",
      "76 2600\n",
      "76 2800\n",
      "76 3000\n",
      "76 3200\n",
      "76 3400\n",
      "Epoch 76: train loss: 0.9086042642593384; test loss: 0.9347676038742065\n",
      "Epoch 76: train accuracy: 0.8262092238470191; test accuracy: 0.8076490438695163\n",
      "77 0\n",
      "77 200\n",
      "77 400\n",
      "77 600\n",
      "77 800\n",
      "77 1000\n",
      "77 1200\n",
      "77 1400\n",
      "77 1600\n",
      "77 1800\n",
      "77 2000\n",
      "77 2200\n",
      "77 2400\n",
      "77 2600\n",
      "77 2800\n",
      "77 3000\n",
      "77 3200\n",
      "77 3400\n",
      "Epoch 77: train loss: 0.908856987953186; test loss: 0.9336532950401306\n",
      "Epoch 77: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "78 0\n",
      "78 200\n",
      "78 400\n",
      "78 600\n",
      "78 800\n",
      "78 1000\n",
      "78 1200\n",
      "78 1400\n",
      "78 1600\n",
      "78 1800\n",
      "78 2000\n",
      "78 2200\n",
      "78 2400\n",
      "78 2600\n",
      "78 2800\n",
      "78 3000\n",
      "78 3200\n",
      "78 3400\n",
      "Epoch 78: train loss: 0.908805251121521; test loss: 0.9333786368370056\n",
      "Epoch 78: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "79 0\n",
      "79 200\n",
      "79 400\n",
      "79 600\n",
      "79 800\n",
      "79 1000\n",
      "79 1200\n",
      "79 1400\n",
      "79 1600\n",
      "79 1800\n",
      "79 2000\n",
      "79 2200\n",
      "79 2400\n",
      "79 2600\n",
      "79 2800\n",
      "79 3000\n",
      "79 3200\n",
      "79 3400\n",
      "Epoch 79: train loss: 0.9082248210906982; test loss: 0.9338307976722717\n",
      "Epoch 79: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "80 0\n",
      "80 200\n",
      "80 400\n",
      "80 600\n",
      "80 800\n",
      "80 1000\n",
      "80 1200\n",
      "80 1400\n",
      "80 1600\n",
      "80 1800\n",
      "80 2000\n",
      "80 2200\n",
      "80 2400\n",
      "80 2600\n",
      "80 2800\n",
      "80 3000\n",
      "80 3200\n",
      "80 3400\n",
      "Epoch 80: train loss: 0.9081640243530273; test loss: 0.9335443377494812\n",
      "Epoch 80: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "81 0\n",
      "81 200\n",
      "81 400\n",
      "81 600\n",
      "81 800\n",
      "81 1000\n",
      "81 1200\n",
      "81 1400\n",
      "81 1600\n",
      "81 1800\n",
      "81 2000\n",
      "81 2200\n",
      "81 2400\n",
      "81 2600\n",
      "81 2800\n",
      "81 3000\n",
      "81 3200\n",
      "81 3400\n",
      "Epoch 81: train loss: 0.9079707264900208; test loss: 0.9345690608024597\n",
      "Epoch 81: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "82 0\n",
      "82 200\n",
      "82 400\n",
      "82 600\n",
      "82 800\n",
      "82 1000\n",
      "82 1200\n",
      "82 1400\n",
      "82 1600\n",
      "82 1800\n",
      "82 2000\n",
      "82 2200\n",
      "82 2400\n",
      "82 2600\n",
      "82 2800\n",
      "82 3000\n",
      "82 3200\n",
      "82 3400\n",
      "Epoch 82: train loss: 0.9078956842422485; test loss: 0.9333401918411255\n",
      "Epoch 82: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "83 0\n",
      "83 200\n",
      "83 400\n",
      "83 600\n",
      "83 800\n",
      "83 1000\n",
      "83 1200\n",
      "83 1400\n",
      "83 1600\n",
      "83 1800\n",
      "83 2000\n",
      "83 2200\n",
      "83 2400\n",
      "83 2600\n",
      "83 2800\n",
      "83 3000\n",
      "83 3200\n",
      "83 3400\n",
      "Epoch 83: train loss: 0.9080818295478821; test loss: 0.934092104434967\n",
      "Epoch 83: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "84 0\n",
      "84 200\n",
      "84 400\n",
      "84 600\n",
      "84 800\n",
      "84 1000\n",
      "84 1200\n",
      "84 1400\n",
      "84 1600\n",
      "84 1800\n",
      "84 2000\n",
      "84 2200\n",
      "84 2400\n",
      "84 2600\n",
      "84 2800\n",
      "84 3000\n",
      "84 3200\n",
      "84 3400\n",
      "Epoch 84: train loss: 0.9083895683288574; test loss: 0.93422532081604\n",
      "Epoch 84: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "85 0\n",
      "85 200\n",
      "85 400\n",
      "85 600\n",
      "85 800\n",
      "85 1000\n",
      "85 1200\n",
      "85 1400\n",
      "85 1600\n",
      "85 1800\n",
      "85 2000\n",
      "85 2200\n",
      "85 2400\n",
      "85 2600\n",
      "85 2800\n",
      "85 3000\n",
      "85 3200\n",
      "85 3400\n",
      "Epoch 85: train loss: 0.9075844883918762; test loss: 0.9340966939926147\n",
      "Epoch 85: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "86 0\n",
      "86 200\n",
      "86 400\n",
      "86 600\n",
      "86 800\n",
      "86 1000\n",
      "86 1200\n",
      "86 1400\n",
      "86 1600\n",
      "86 1800\n",
      "86 2000\n",
      "86 2200\n",
      "86 2400\n",
      "86 2600\n",
      "86 2800\n",
      "86 3000\n",
      "86 3200\n",
      "86 3400\n",
      "Epoch 86: train loss: 0.9078280329704285; test loss: 0.9338008165359497\n",
      "Epoch 86: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "87 0\n",
      "87 200\n",
      "87 400\n",
      "87 600\n",
      "87 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 1000\n",
      "87 1200\n",
      "87 1400\n",
      "87 1600\n",
      "87 1800\n",
      "87 2000\n",
      "87 2200\n",
      "87 2400\n",
      "87 2600\n",
      "87 2800\n",
      "87 3000\n",
      "87 3200\n",
      "87 3400\n",
      "Epoch 87: train loss: 0.9076029658317566; test loss: 0.9333291053771973\n",
      "Epoch 87: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "88 0\n",
      "88 200\n",
      "88 400\n",
      "88 600\n",
      "88 800\n",
      "88 1000\n",
      "88 1200\n",
      "88 1400\n",
      "88 1600\n",
      "88 1800\n",
      "88 2000\n",
      "88 2200\n",
      "88 2400\n",
      "88 2600\n",
      "88 2800\n",
      "88 3000\n",
      "88 3200\n",
      "88 3400\n",
      "Epoch 88: train loss: 0.9074360728263855; test loss: 0.934195876121521\n",
      "Epoch 88: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "89 0\n",
      "89 200\n",
      "89 400\n",
      "89 600\n",
      "89 800\n",
      "89 1000\n",
      "89 1200\n",
      "89 1400\n",
      "89 1600\n",
      "89 1800\n",
      "89 2000\n",
      "89 2200\n",
      "89 2400\n",
      "89 2600\n",
      "89 2800\n",
      "89 3000\n",
      "89 3200\n",
      "89 3400\n",
      "Epoch 89: train loss: 0.908097505569458; test loss: 0.9336177110671997\n",
      "Epoch 89: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "90 0\n",
      "90 200\n",
      "90 400\n",
      "90 600\n",
      "90 800\n",
      "90 1000\n",
      "90 1200\n",
      "90 1400\n",
      "90 1600\n",
      "90 1800\n",
      "90 2000\n",
      "90 2200\n",
      "90 2400\n",
      "90 2600\n",
      "90 2800\n",
      "90 3000\n",
      "90 3200\n",
      "90 3400\n",
      "Epoch 90: train loss: 0.9085283279418945; test loss: 0.9335950016975403\n",
      "Epoch 90: train accuracy: 0.827615298087739; test accuracy: 0.8087739032620922\n",
      "91 0\n",
      "91 200\n",
      "91 400\n",
      "91 600\n",
      "91 800\n",
      "91 1000\n",
      "91 1200\n",
      "91 1400\n",
      "91 1600\n",
      "91 1800\n",
      "91 2000\n",
      "91 2200\n",
      "91 2400\n",
      "91 2600\n",
      "91 2800\n",
      "91 3000\n",
      "91 3200\n",
      "91 3400\n",
      "Epoch 91: train loss: 0.9110337495803833; test loss: 0.9364349246025085\n",
      "Epoch 91: train accuracy: 0.8256467941507312; test accuracy: 0.8065241844769404\n",
      "92 0\n",
      "92 200\n",
      "92 400\n",
      "92 600\n",
      "92 800\n",
      "92 1000\n",
      "92 1200\n",
      "92 1400\n",
      "92 1600\n",
      "92 1800\n",
      "92 2000\n",
      "92 2200\n",
      "92 2400\n",
      "92 2600\n",
      "92 2800\n",
      "92 3000\n",
      "92 3200\n",
      "92 3400\n",
      "Epoch 92: train loss: 0.9146702885627747; test loss: 0.9361621141433716\n",
      "Epoch 92: train accuracy: 0.8256467941507312; test accuracy: 0.8065241844769404\n",
      "93 0\n",
      "93 200\n",
      "93 400\n",
      "93 600\n",
      "93 800\n",
      "93 1000\n",
      "93 1200\n",
      "93 1400\n",
      "93 1600\n",
      "93 1800\n",
      "93 2000\n",
      "93 2200\n",
      "93 2400\n",
      "93 2600\n",
      "93 2800\n",
      "93 3000\n",
      "93 3200\n",
      "93 3400\n",
      "Epoch 93: train loss: 0.9138516187667847; test loss: 0.9366860389709473\n",
      "Epoch 93: train accuracy: 0.8245219347581553; test accuracy: 0.8042744656917885\n",
      "94 0\n",
      "94 200\n",
      "94 400\n",
      "94 600\n",
      "94 800\n",
      "94 1000\n",
      "94 1200\n",
      "94 1400\n",
      "94 1600\n",
      "94 1800\n",
      "94 2000\n",
      "94 2200\n",
      "94 2400\n",
      "94 2600\n",
      "94 2800\n",
      "94 3000\n",
      "94 3200\n",
      "94 3400\n",
      "Epoch 94: train loss: 0.9350380301475525; test loss: 0.9384765625\n",
      "Epoch 94: train accuracy: 0.8245219347581553; test accuracy: 0.8042744656917885\n",
      "95 0\n",
      "95 200\n",
      "95 400\n",
      "95 600\n",
      "95 800\n",
      "95 1000\n",
      "95 1200\n",
      "95 1400\n",
      "95 1600\n",
      "95 1800\n",
      "95 2000\n",
      "95 2200\n",
      "95 2400\n",
      "95 2600\n",
      "95 2800\n",
      "95 3000\n",
      "95 3200\n",
      "95 3400\n",
      "Epoch 95: train loss: 0.9102279543876648; test loss: 0.9388234615325928\n",
      "Epoch 95: train accuracy: 0.8245219347581553; test accuracy: 0.8065241844769404\n",
      "96 0\n",
      "96 200\n",
      "96 400\n",
      "96 600\n",
      "96 800\n",
      "96 1000\n",
      "96 1200\n",
      "96 1400\n",
      "96 1600\n",
      "96 1800\n",
      "96 2000\n",
      "96 2200\n",
      "96 2400\n",
      "96 2600\n",
      "96 2800\n",
      "96 3000\n",
      "96 3200\n",
      "96 3400\n",
      "Epoch 96: train loss: 0.9234780669212341; test loss: 0.9403027892112732\n",
      "Epoch 96: train accuracy: 0.8239595050618672; test accuracy: 0.8042744656917885\n",
      "97 0\n",
      "97 200\n",
      "97 400\n",
      "97 600\n",
      "97 800\n",
      "97 1000\n",
      "97 1200\n",
      "97 1400\n",
      "97 1600\n",
      "97 1800\n",
      "97 2000\n",
      "97 2200\n",
      "97 2400\n",
      "97 2600\n",
      "97 2800\n",
      "97 3000\n",
      "97 3200\n",
      "97 3400\n",
      "Epoch 97: train loss: 0.9267202615737915; test loss: 0.9394558072090149\n",
      "Epoch 97: train accuracy: 0.8225534308211474; test accuracy: 0.8053993250843644\n",
      "98 0\n",
      "98 200\n",
      "98 400\n",
      "98 600\n",
      "98 800\n",
      "98 1000\n",
      "98 1200\n",
      "98 1400\n",
      "98 1600\n",
      "98 1800\n",
      "98 2000\n",
      "98 2200\n",
      "98 2400\n",
      "98 2600\n",
      "98 2800\n",
      "98 3000\n",
      "98 3200\n",
      "98 3400\n",
      "Epoch 98: train loss: 0.9212868809700012; test loss: 0.9354206919670105\n",
      "Epoch 98: train accuracy: 0.8250843644544432; test accuracy: 0.8065241844769404\n",
      "99 0\n",
      "99 200\n",
      "99 400\n",
      "99 600\n",
      "99 800\n",
      "99 1000\n",
      "99 1200\n",
      "99 1400\n",
      "99 1600\n",
      "99 1800\n",
      "99 2000\n",
      "99 2200\n",
      "99 2400\n",
      "99 2600\n",
      "99 2800\n",
      "99 3000\n",
      "99 3200\n",
      "99 3400\n",
      "Epoch 99: train loss: 0.9054186940193176; test loss: 0.9287201166152954\n",
      "Epoch 99: train accuracy: 0.8262092238470191; test accuracy: 0.8143982002249719\n",
      "100 0\n",
      "100 200\n",
      "100 400\n",
      "100 600\n",
      "100 800\n",
      "100 1000\n",
      "100 1200\n",
      "100 1400\n",
      "100 1600\n",
      "100 1800\n",
      "100 2000\n",
      "100 2200\n",
      "100 2400\n",
      "100 2600\n",
      "100 2800\n",
      "100 3000\n",
      "100 3200\n",
      "100 3400\n",
      "Epoch 100: train loss: 0.9054793119430542; test loss: 0.9272541403770447\n",
      "Epoch 100: train accuracy: 0.827334083239595; test accuracy: 0.8166479190101237\n",
      "101 0\n",
      "101 200\n",
      "101 400\n",
      "101 600\n",
      "101 800\n",
      "101 1000\n",
      "101 1200\n",
      "101 1400\n",
      "101 1600\n",
      "101 1800\n",
      "101 2000\n",
      "101 2200\n",
      "101 2400\n",
      "101 2600\n",
      "101 2800\n",
      "101 3000\n",
      "101 3200\n",
      "101 3400\n",
      "Epoch 101: train loss: 0.9061421155929565; test loss: 0.9333750605583191\n",
      "Epoch 101: train accuracy: 0.8253655793025871; test accuracy: 0.8098987626546682\n",
      "102 0\n",
      "102 200\n",
      "102 400\n",
      "102 600\n",
      "102 800\n",
      "102 1000\n",
      "102 1200\n",
      "102 1400\n",
      "102 1600\n",
      "102 1800\n",
      "102 2000\n",
      "102 2200\n",
      "102 2400\n",
      "102 2600\n",
      "102 2800\n",
      "102 3000\n",
      "102 3200\n",
      "102 3400\n",
      "Epoch 102: train loss: 0.9024611115455627; test loss: 0.9291346073150635\n",
      "Epoch 102: train accuracy: 0.827334083239595; test accuracy: 0.8143982002249719\n",
      "103 0\n",
      "103 200\n",
      "103 400\n",
      "103 600\n",
      "103 800\n",
      "103 1000\n",
      "103 1200\n",
      "103 1400\n",
      "103 1600\n",
      "103 1800\n",
      "103 2000\n",
      "103 2200\n",
      "103 2400\n",
      "103 2600\n",
      "103 2800\n",
      "103 3000\n",
      "103 3200\n",
      "103 3400\n",
      "Epoch 103: train loss: 0.9043989181518555; test loss: 0.9302597641944885\n",
      "Epoch 103: train accuracy: 0.827334083239595; test accuracy: 0.81214848143982\n",
      "104 0\n",
      "104 200\n",
      "104 400\n",
      "104 600\n",
      "104 800\n",
      "104 1000\n",
      "104 1200\n",
      "104 1400\n",
      "104 1600\n",
      "104 1800\n",
      "104 2000\n",
      "104 2200\n",
      "104 2400\n",
      "104 2600\n",
      "104 2800\n",
      "104 3000\n",
      "104 3200\n",
      "104 3400\n",
      "Epoch 104: train loss: 0.9018111824989319; test loss: 0.9324954152107239\n",
      "Epoch 104: train accuracy: 0.8253655793025871; test accuracy: 0.8076490438695163\n",
      "105 0\n",
      "105 200\n",
      "105 400\n",
      "105 600\n",
      "105 800\n",
      "105 1000\n",
      "105 1200\n",
      "105 1400\n",
      "105 1600\n",
      "105 1800\n",
      "105 2000\n",
      "105 2200\n",
      "105 2400\n",
      "105 2600\n",
      "105 2800\n",
      "105 3000\n",
      "105 3200\n",
      "105 3400\n",
      "Epoch 105: train loss: 0.903436541557312; test loss: 0.9310230016708374\n",
      "Epoch 105: train accuracy: 0.8262092238470191; test accuracy: 0.81214848143982\n",
      "106 0\n",
      "106 200\n",
      "106 400\n",
      "106 600\n",
      "106 800\n",
      "106 1000\n",
      "106 1200\n",
      "106 1400\n",
      "106 1600\n",
      "106 1800\n",
      "106 2000\n",
      "106 2200\n",
      "106 2400\n",
      "106 2600\n",
      "106 2800\n",
      "106 3000\n",
      "106 3200\n",
      "106 3400\n",
      "Epoch 106: train loss: 0.9024113416671753; test loss: 0.9273386597633362\n",
      "Epoch 106: train accuracy: 0.827334083239595; test accuracy: 0.8143982002249719\n",
      "107 0\n",
      "107 200\n",
      "107 400\n",
      "107 600\n",
      "107 800\n",
      "107 1000\n",
      "107 1200\n",
      "107 1400\n",
      "107 1600\n",
      "107 1800\n",
      "107 2000\n",
      "107 2200\n",
      "107 2400\n",
      "107 2600\n",
      "107 2800\n",
      "107 3000\n",
      "107 3200\n",
      "107 3400\n",
      "Epoch 107: train loss: 0.9011703133583069; test loss: 0.9275253415107727\n",
      "Epoch 107: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "108 0\n",
      "108 200\n",
      "108 400\n",
      "108 600\n",
      "108 800\n",
      "108 1000\n",
      "108 1200\n",
      "108 1400\n",
      "108 1600\n",
      "108 1800\n",
      "108 2000\n",
      "108 2200\n",
      "108 2400\n",
      "108 2600\n",
      "108 2800\n",
      "108 3000\n",
      "108 3200\n",
      "108 3400\n",
      "Epoch 108: train loss: 0.9018865823745728; test loss: 0.9273558855056763\n",
      "Epoch 108: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "109 0\n",
      "109 200\n",
      "109 400\n",
      "109 600\n",
      "109 800\n",
      "109 1000\n",
      "109 1200\n",
      "109 1400\n",
      "109 1600\n",
      "109 1800\n",
      "109 2000\n",
      "109 2200\n",
      "109 2400\n",
      "109 2600\n",
      "109 2800\n",
      "109 3000\n",
      "109 3200\n",
      "109 3400\n",
      "Epoch 109: train loss: 0.9020193815231323; test loss: 0.9284106492996216\n",
      "Epoch 109: train accuracy: 0.827334083239595; test accuracy: 0.8143982002249719\n",
      "110 0\n",
      "110 200\n",
      "110 400\n",
      "110 600\n",
      "110 800\n",
      "110 1000\n",
      "110 1200\n",
      "110 1400\n",
      "110 1600\n",
      "110 1800\n",
      "110 2000\n",
      "110 2200\n",
      "110 2400\n",
      "110 2600\n",
      "110 2800\n",
      "110 3000\n",
      "110 3200\n",
      "110 3400\n",
      "Epoch 110: train loss: 0.9013345241546631; test loss: 0.9278764128684998\n",
      "Epoch 110: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "111 0\n",
      "111 200\n",
      "111 400\n",
      "111 600\n",
      "111 800\n",
      "111 1000\n",
      "111 1200\n",
      "111 1400\n",
      "111 1600\n",
      "111 1800\n",
      "111 2000\n",
      "111 2200\n",
      "111 2400\n",
      "111 2600\n",
      "111 2800\n",
      "111 3000\n",
      "111 3200\n",
      "111 3400\n",
      "Epoch 111: train loss: 0.9017438888549805; test loss: 0.9277788400650024\n",
      "Epoch 111: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "112 0\n",
      "112 200\n",
      "112 400\n",
      "112 600\n",
      "112 800\n",
      "112 1000\n",
      "112 1200\n",
      "112 1400\n",
      "112 1600\n",
      "112 1800\n",
      "112 2000\n",
      "112 2200\n",
      "112 2400\n",
      "112 2600\n",
      "112 2800\n",
      "112 3000\n",
      "112 3200\n",
      "112 3400\n",
      "Epoch 112: train loss: 0.9018125534057617; test loss: 0.928074300289154\n",
      "Epoch 112: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "113 0\n",
      "113 200\n",
      "113 400\n",
      "113 600\n",
      "113 800\n",
      "113 1000\n",
      "113 1200\n",
      "113 1400\n",
      "113 1600\n",
      "113 1800\n",
      "113 2000\n",
      "113 2200\n",
      "113 2400\n",
      "113 2600\n",
      "113 2800\n",
      "113 3000\n",
      "113 3200\n",
      "113 3400\n",
      "Epoch 113: train loss: 0.901426374912262; test loss: 0.9274261593818665\n",
      "Epoch 113: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "114 0\n",
      "114 200\n",
      "114 400\n",
      "114 600\n",
      "114 800\n",
      "114 1000\n",
      "114 1200\n",
      "114 1400\n",
      "114 1600\n",
      "114 1800\n",
      "114 2000\n",
      "114 2200\n",
      "114 2400\n",
      "114 2600\n",
      "114 2800\n",
      "114 3000\n",
      "114 3200\n",
      "114 3400\n",
      "Epoch 114: train loss: 0.9017579555511475; test loss: 0.9284939765930176\n",
      "Epoch 114: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "115 0\n",
      "115 200\n",
      "115 400\n",
      "115 600\n",
      "115 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115 1000\n",
      "115 1200\n",
      "115 1400\n",
      "115 1600\n",
      "115 1800\n",
      "115 2000\n",
      "115 2200\n",
      "115 2400\n",
      "115 2600\n",
      "115 2800\n",
      "115 3000\n",
      "115 3200\n",
      "115 3400\n",
      "Epoch 115: train loss: 0.901252269744873; test loss: 0.9276766777038574\n",
      "Epoch 115: train accuracy: 0.828458942632171; test accuracy: 0.8143982002249719\n",
      "116 0\n",
      "116 200\n",
      "116 400\n",
      "116 600\n",
      "116 800\n",
      "116 1000\n",
      "116 1200\n",
      "116 1400\n",
      "116 1600\n",
      "116 1800\n",
      "116 2000\n",
      "116 2200\n",
      "116 2400\n",
      "116 2600\n",
      "116 2800\n",
      "116 3000\n",
      "116 3200\n",
      "116 3400\n",
      "Epoch 116: train loss: 0.9017642140388489; test loss: 0.9294067621231079\n",
      "Epoch 116: train accuracy: 0.829021372328459; test accuracy: 0.8143982002249719\n",
      "117 0\n",
      "117 200\n",
      "117 400\n",
      "117 600\n",
      "117 800\n",
      "117 1000\n",
      "117 1200\n",
      "117 1400\n",
      "117 1600\n",
      "117 1800\n",
      "117 2000\n",
      "117 2200\n",
      "117 2400\n",
      "117 2600\n",
      "117 2800\n",
      "117 3000\n",
      "117 3200\n",
      "117 3400\n",
      "Epoch 117: train loss: 0.9024845957756042; test loss: 0.9311139583587646\n",
      "Epoch 117: train accuracy: 0.8295838020247469; test accuracy: 0.813273340832396\n",
      "118 0\n",
      "118 200\n",
      "118 400\n",
      "118 600\n",
      "118 800\n",
      "118 1000\n",
      "118 1200\n",
      "118 1400\n",
      "118 1600\n",
      "118 1800\n",
      "118 2000\n",
      "118 2200\n",
      "118 2400\n",
      "118 2600\n",
      "118 2800\n",
      "118 3000\n",
      "118 3200\n",
      "118 3400\n",
      "Epoch 118: train loss: 0.9006943106651306; test loss: 0.9245853424072266\n",
      "Epoch 118: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "119 0\n",
      "119 200\n",
      "119 400\n",
      "119 600\n",
      "119 800\n",
      "119 1000\n",
      "119 1200\n",
      "119 1400\n",
      "119 1600\n",
      "119 1800\n",
      "119 2000\n",
      "119 2200\n",
      "119 2400\n",
      "119 2600\n",
      "119 2800\n",
      "119 3000\n",
      "119 3200\n",
      "119 3400\n",
      "Epoch 119: train loss: 0.9013043642044067; test loss: 0.9244609475135803\n",
      "Epoch 119: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "120 0\n",
      "120 200\n",
      "120 400\n",
      "120 600\n",
      "120 800\n",
      "120 1000\n",
      "120 1200\n",
      "120 1400\n",
      "120 1600\n",
      "120 1800\n",
      "120 2000\n",
      "120 2200\n",
      "120 2400\n",
      "120 2600\n",
      "120 2800\n",
      "120 3000\n",
      "120 3200\n",
      "120 3400\n",
      "Epoch 120: train loss: 0.9016690254211426; test loss: 0.9246200919151306\n",
      "Epoch 120: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "121 0\n",
      "121 200\n",
      "121 400\n",
      "121 600\n",
      "121 800\n",
      "121 1000\n",
      "121 1200\n",
      "121 1400\n",
      "121 1600\n",
      "121 1800\n",
      "121 2000\n",
      "121 2200\n",
      "121 2400\n",
      "121 2600\n",
      "121 2800\n",
      "121 3000\n",
      "121 3200\n",
      "121 3400\n",
      "Epoch 121: train loss: 0.9010124206542969; test loss: 0.9246612191200256\n",
      "Epoch 121: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "122 0\n",
      "122 200\n",
      "122 400\n",
      "122 600\n",
      "122 800\n",
      "122 1000\n",
      "122 1200\n",
      "122 1400\n",
      "122 1600\n",
      "122 1800\n",
      "122 2000\n",
      "122 2200\n",
      "122 2400\n",
      "122 2600\n",
      "122 2800\n",
      "122 3000\n",
      "122 3200\n",
      "122 3400\n",
      "Epoch 122: train loss: 0.9012454152107239; test loss: 0.9245696067810059\n",
      "Epoch 122: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "123 0\n",
      "123 200\n",
      "123 400\n",
      "123 600\n",
      "123 800\n",
      "123 1000\n",
      "123 1200\n",
      "123 1400\n",
      "123 1600\n",
      "123 1800\n",
      "123 2000\n",
      "123 2200\n",
      "123 2400\n",
      "123 2600\n",
      "123 2800\n",
      "123 3000\n",
      "123 3200\n",
      "123 3400\n",
      "Epoch 123: train loss: 0.9010983109474182; test loss: 0.9244005084037781\n",
      "Epoch 123: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "124 0\n",
      "124 200\n",
      "124 400\n",
      "124 600\n",
      "124 800\n",
      "124 1000\n",
      "124 1200\n",
      "124 1400\n",
      "124 1600\n",
      "124 1800\n",
      "124 2000\n",
      "124 2200\n",
      "124 2400\n",
      "124 2600\n",
      "124 2800\n",
      "124 3000\n",
      "124 3200\n",
      "124 3400\n",
      "Epoch 124: train loss: 0.9008389115333557; test loss: 0.9242070913314819\n",
      "Epoch 124: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "125 0\n",
      "125 200\n",
      "125 400\n",
      "125 600\n",
      "125 800\n",
      "125 1000\n",
      "125 1200\n",
      "125 1400\n",
      "125 1600\n",
      "125 1800\n",
      "125 2000\n",
      "125 2200\n",
      "125 2400\n",
      "125 2600\n",
      "125 2800\n",
      "125 3000\n",
      "125 3200\n",
      "125 3400\n",
      "Epoch 125: train loss: 0.9019065499305725; test loss: 0.9243796467781067\n",
      "Epoch 125: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "126 0\n",
      "126 200\n",
      "126 400\n",
      "126 600\n",
      "126 800\n",
      "126 1000\n",
      "126 1200\n",
      "126 1400\n",
      "126 1600\n",
      "126 1800\n",
      "126 2000\n",
      "126 2200\n",
      "126 2400\n",
      "126 2600\n",
      "126 2800\n",
      "126 3000\n",
      "126 3200\n",
      "126 3400\n",
      "Epoch 126: train loss: 0.9019588828086853; test loss: 0.9255142211914062\n",
      "Epoch 126: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "127 0\n",
      "127 200\n",
      "127 400\n",
      "127 600\n",
      "127 800\n",
      "127 1000\n",
      "127 1200\n",
      "127 1400\n",
      "127 1600\n",
      "127 1800\n",
      "127 2000\n",
      "127 2200\n",
      "127 2400\n",
      "127 2600\n",
      "127 2800\n",
      "127 3000\n",
      "127 3200\n",
      "127 3400\n",
      "Epoch 127: train loss: 0.9026527404785156; test loss: 0.9277986288070679\n",
      "Epoch 127: train accuracy: 0.8295838020247469; test accuracy: 0.8155230596175478\n",
      "128 0\n",
      "128 200\n",
      "128 400\n",
      "128 600\n",
      "128 800\n",
      "128 1000\n",
      "128 1200\n",
      "128 1400\n",
      "128 1600\n",
      "128 1800\n",
      "128 2000\n",
      "128 2200\n",
      "128 2400\n",
      "128 2600\n",
      "128 2800\n",
      "128 3000\n",
      "128 3200\n",
      "128 3400\n",
      "Epoch 128: train loss: 0.9006547331809998; test loss: 0.9252821803092957\n",
      "Epoch 128: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "129 0\n",
      "129 200\n",
      "129 400\n",
      "129 600\n",
      "129 800\n",
      "129 1000\n",
      "129 1200\n",
      "129 1400\n",
      "129 1600\n",
      "129 1800\n",
      "129 2000\n",
      "129 2200\n",
      "129 2400\n",
      "129 2600\n",
      "129 2800\n",
      "129 3000\n",
      "129 3200\n",
      "129 3400\n",
      "Epoch 129: train loss: 0.9006768465042114; test loss: 0.9247917532920837\n",
      "Epoch 129: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "130 0\n",
      "130 200\n",
      "130 400\n",
      "130 600\n",
      "130 800\n",
      "130 1000\n",
      "130 1200\n",
      "130 1400\n",
      "130 1600\n",
      "130 1800\n",
      "130 2000\n",
      "130 2200\n",
      "130 2400\n",
      "130 2600\n",
      "130 2800\n",
      "130 3000\n",
      "130 3200\n",
      "130 3400\n",
      "Epoch 130: train loss: 0.9013501405715942; test loss: 0.9249511361122131\n",
      "Epoch 130: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "131 0\n",
      "131 200\n",
      "131 400\n",
      "131 600\n",
      "131 800\n",
      "131 1000\n",
      "131 1200\n",
      "131 1400\n",
      "131 1600\n",
      "131 1800\n",
      "131 2000\n",
      "131 2200\n",
      "131 2400\n",
      "131 2600\n",
      "131 2800\n",
      "131 3000\n",
      "131 3200\n",
      "131 3400\n",
      "Epoch 131: train loss: 0.9008065462112427; test loss: 0.9246259331703186\n",
      "Epoch 131: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "132 0\n",
      "132 200\n",
      "132 400\n",
      "132 600\n",
      "132 800\n",
      "132 1000\n",
      "132 1200\n",
      "132 1400\n",
      "132 1600\n",
      "132 1800\n",
      "132 2000\n",
      "132 2200\n",
      "132 2400\n",
      "132 2600\n",
      "132 2800\n",
      "132 3000\n",
      "132 3200\n",
      "132 3400\n",
      "Epoch 132: train loss: 0.9017156958580017; test loss: 0.924734354019165\n",
      "Epoch 132: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "133 0\n",
      "133 200\n",
      "133 400\n",
      "133 600\n",
      "133 800\n",
      "133 1000\n",
      "133 1200\n",
      "133 1400\n",
      "133 1600\n",
      "133 1800\n",
      "133 2000\n",
      "133 2200\n",
      "133 2400\n",
      "133 2600\n",
      "133 2800\n",
      "133 3000\n",
      "133 3200\n",
      "133 3400\n",
      "Epoch 133: train loss: 0.9012858271598816; test loss: 0.9244734048843384\n",
      "Epoch 133: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "134 0\n",
      "134 200\n",
      "134 400\n",
      "134 600\n",
      "134 800\n",
      "134 1000\n",
      "134 1200\n",
      "134 1400\n",
      "134 1600\n",
      "134 1800\n",
      "134 2000\n",
      "134 2200\n",
      "134 2400\n",
      "134 2600\n",
      "134 2800\n",
      "134 3000\n",
      "134 3200\n",
      "134 3400\n",
      "Epoch 134: train loss: 0.9011601209640503; test loss: 0.9239866733551025\n",
      "Epoch 134: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "135 0\n",
      "135 200\n",
      "135 400\n",
      "135 600\n",
      "135 800\n",
      "135 1000\n",
      "135 1200\n",
      "135 1400\n",
      "135 1600\n",
      "135 1800\n",
      "135 2000\n",
      "135 2200\n",
      "135 2400\n",
      "135 2600\n",
      "135 2800\n",
      "135 3000\n",
      "135 3200\n",
      "135 3400\n",
      "Epoch 135: train loss: 0.9015697836875916; test loss: 0.9241455793380737\n",
      "Epoch 135: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "136 0\n",
      "136 200\n",
      "136 400\n",
      "136 600\n",
      "136 800\n",
      "136 1000\n",
      "136 1200\n",
      "136 1400\n",
      "136 1600\n",
      "136 1800\n",
      "136 2000\n",
      "136 2200\n",
      "136 2400\n",
      "136 2600\n",
      "136 2800\n",
      "136 3000\n",
      "136 3200\n",
      "136 3400\n",
      "Epoch 136: train loss: 0.9020954966545105; test loss: 0.9274308681488037\n",
      "Epoch 136: train accuracy: 0.8307086614173228; test accuracy: 0.8155230596175478\n",
      "137 0\n",
      "137 200\n",
      "137 400\n",
      "137 600\n",
      "137 800\n",
      "137 1000\n",
      "137 1200\n",
      "137 1400\n",
      "137 1600\n",
      "137 1800\n",
      "137 2000\n",
      "137 2200\n",
      "137 2400\n",
      "137 2600\n",
      "137 2800\n",
      "137 3000\n",
      "137 3200\n",
      "137 3400\n",
      "Epoch 137: train loss: 0.9012553691864014; test loss: 0.9266789555549622\n",
      "Epoch 137: train accuracy: 0.8304274465691789; test accuracy: 0.8143982002249719\n",
      "138 0\n",
      "138 200\n",
      "138 400\n",
      "138 600\n",
      "138 800\n",
      "138 1000\n",
      "138 1200\n",
      "138 1400\n",
      "138 1600\n",
      "138 1800\n",
      "138 2000\n",
      "138 2200\n",
      "138 2400\n",
      "138 2600\n",
      "138 2800\n",
      "138 3000\n",
      "138 3200\n",
      "138 3400\n",
      "Epoch 138: train loss: 0.9019836783409119; test loss: 0.9248208999633789\n",
      "Epoch 138: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "139 0\n",
      "139 200\n",
      "139 400\n",
      "139 600\n",
      "139 800\n",
      "139 1000\n",
      "139 1200\n",
      "139 1400\n",
      "139 1600\n",
      "139 1800\n",
      "139 2000\n",
      "139 2200\n",
      "139 2400\n",
      "139 2600\n",
      "139 2800\n",
      "139 3000\n",
      "139 3200\n",
      "139 3400\n",
      "Epoch 139: train loss: 0.9010650515556335; test loss: 0.9239206314086914\n",
      "Epoch 139: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "140 0\n",
      "140 200\n",
      "140 400\n",
      "140 600\n",
      "140 800\n",
      "140 1000\n",
      "140 1200\n",
      "140 1400\n",
      "140 1600\n",
      "140 1800\n",
      "140 2000\n",
      "140 2200\n",
      "140 2400\n",
      "140 2600\n",
      "140 2800\n",
      "140 3000\n",
      "140 3200\n",
      "140 3400\n",
      "Epoch 140: train loss: 0.9012799263000488; test loss: 0.924505889415741\n",
      "Epoch 140: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "141 0\n",
      "141 200\n",
      "141 400\n",
      "141 600\n",
      "141 800\n",
      "141 1000\n",
      "141 1200\n",
      "141 1400\n",
      "141 1600\n",
      "141 1800\n",
      "141 2000\n",
      "141 2200\n",
      "141 2400\n",
      "141 2600\n",
      "141 2800\n",
      "141 3000\n",
      "141 3200\n",
      "141 3400\n",
      "Epoch 141: train loss: 0.9013238549232483; test loss: 0.9250078201293945\n",
      "Epoch 141: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "142 0\n",
      "142 200\n",
      "142 400\n",
      "142 600\n",
      "142 800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142 1000\n",
      "142 1200\n",
      "142 1400\n",
      "142 1600\n",
      "142 1800\n",
      "142 2000\n",
      "142 2200\n",
      "142 2400\n",
      "142 2600\n",
      "142 2800\n",
      "142 3000\n",
      "142 3200\n",
      "142 3400\n",
      "Epoch 142: train loss: 0.9006259441375732; test loss: 0.9241757392883301\n",
      "Epoch 142: train accuracy: 0.8318335208098988; test accuracy: 0.8177727784026997\n",
      "143 0\n",
      "143 200\n",
      "143 400\n",
      "143 600\n",
      "143 800\n",
      "143 1000\n",
      "143 1200\n",
      "143 1400\n",
      "143 1600\n",
      "143 1800\n",
      "143 2000\n",
      "143 2200\n",
      "143 2400\n",
      "143 2600\n",
      "143 2800\n",
      "143 3000\n",
      "143 3200\n",
      "143 3400\n",
      "Epoch 143: train loss: 0.9012117385864258; test loss: 0.9228833913803101\n",
      "Epoch 143: train accuracy: 0.8326771653543307; test accuracy: 0.8188976377952756\n",
      "144 0\n",
      "144 200\n",
      "144 400\n",
      "144 600\n",
      "144 800\n",
      "144 1000\n",
      "144 1200\n",
      "144 1400\n",
      "144 1600\n",
      "144 1800\n",
      "144 2000\n",
      "144 2200\n",
      "144 2400\n",
      "144 2600\n",
      "144 2800\n",
      "144 3000\n",
      "144 3200\n",
      "144 3400\n",
      "Epoch 144: train loss: 0.9018985033035278; test loss: 0.9241146445274353\n",
      "Epoch 144: train accuracy: 0.8326771653543307; test accuracy: 0.8188976377952756\n",
      "145 0\n",
      "145 200\n",
      "145 400\n",
      "145 600\n",
      "145 800\n",
      "145 1000\n",
      "145 1200\n",
      "145 1400\n",
      "145 1600\n",
      "145 1800\n",
      "145 2000\n",
      "145 2200\n",
      "145 2400\n",
      "145 2600\n",
      "145 2800\n",
      "145 3000\n",
      "145 3200\n",
      "145 3400\n",
      "Epoch 145: train loss: 0.9022480845451355; test loss: 0.923198401927948\n",
      "Epoch 145: train accuracy: 0.8326771653543307; test accuracy: 0.8188976377952756\n",
      "146 0\n",
      "146 200\n",
      "146 400\n",
      "146 600\n",
      "146 800\n",
      "146 1000\n",
      "146 1200\n",
      "146 1400\n",
      "146 1600\n",
      "146 1800\n",
      "146 2000\n",
      "146 2200\n",
      "146 2400\n",
      "146 2600\n",
      "146 2800\n",
      "146 3000\n",
      "146 3200\n",
      "146 3400\n",
      "Epoch 146: train loss: 0.9038940668106079; test loss: 0.924040675163269\n",
      "Epoch 146: train accuracy: 0.8326771653543307; test accuracy: 0.8188976377952756\n",
      "147 0\n",
      "147 200\n",
      "147 400\n",
      "147 600\n",
      "147 800\n",
      "147 1000\n",
      "147 1200\n",
      "147 1400\n",
      "147 1600\n",
      "147 1800\n",
      "147 2000\n",
      "147 2200\n",
      "147 2400\n",
      "147 2600\n",
      "147 2800\n",
      "147 3000\n",
      "147 3200\n",
      "147 3400\n",
      "Epoch 147: train loss: 0.9231297373771667; test loss: 0.9398174285888672\n",
      "Epoch 147: train accuracy: 0.8152418447694039; test accuracy: 0.8020247469066367\n",
      "148 0\n",
      "148 200\n",
      "148 400\n",
      "148 600\n",
      "148 800\n",
      "148 1000\n",
      "148 1200\n",
      "148 1400\n",
      "148 1600\n",
      "148 1800\n",
      "148 2000\n",
      "148 2200\n",
      "148 2400\n",
      "148 2600\n",
      "148 2800\n",
      "148 3000\n",
      "148 3200\n",
      "148 3400\n",
      "Epoch 148: train loss: 0.905107855796814; test loss: 0.9243132472038269\n",
      "Epoch 148: train accuracy: 0.8326771653543307; test accuracy: 0.8211473565804275\n",
      "149 0\n",
      "149 200\n",
      "149 400\n",
      "149 600\n",
      "149 800\n",
      "149 1000\n",
      "149 1200\n",
      "149 1400\n",
      "149 1600\n",
      "149 1800\n",
      "149 2000\n",
      "149 2200\n",
      "149 2400\n",
      "149 2600\n",
      "149 2800\n",
      "149 3000\n",
      "149 3200\n",
      "149 3400\n",
      "Epoch 149: train loss: 0.9063147306442261; test loss: 0.9231553673744202\n",
      "Epoch 149: train accuracy: 0.8326771653543307; test accuracy: 0.8211473565804275\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "for n_epoch in range(n_epoches):\n",
    "    # create permutation for batch training\n",
    "    # To-Do: add permutation for SGD...But it is slow.\n",
    "#    permutation = torch.randperm(x_train_images_norm_torch.size()[0])\n",
    "    for i in range(0, x_train_images_norm_torch.size()[0], batch_size):\n",
    "        print(n_epoch, i)\n",
    "        # clear gradients first (for each iteration!)!\n",
    "        optim.zero_grad()\n",
    "        # forward pass\n",
    "        batch_x_image, batch_y = x_train_images_norm_torch[i:i+batch_size, :, :, :].to(device), y_train_torch[i:i+batch_size].to(device)\n",
    "        \n",
    "        batch_x_nhts = x_train_nhts_torch[i:i+batch_size].to(device)\n",
    "        \n",
    "        batch_y_pred_train = combined_net(batch_x_image.to(device), batch_x_nhts.to(device))\n",
    "        # loss \n",
    "        loss = criterion(batch_y_pred_train.squeeze(), batch_y)\n",
    "        # compute gradients\n",
    "        loss.backward()\n",
    "        # one step optim\n",
    "        optim.step()\n",
    "\n",
    "    # eval training accuracy\n",
    "    with torch.no_grad():\n",
    "        y_pred_train = combined_net(x_train_images_norm_torch.to(device),x_train_nhts_torch.to(device))\n",
    "        loss_train = criterion(y_pred_train.squeeze(), y_train_torch.to(device))\n",
    "        train_losses.append(loss_train)\n",
    "        _, predict_train = torch.max(y_pred_train, axis = 1)\n",
    "        accuracy_train = (predict_train == y_train_torch.to(device)).sum().item()/n_train\n",
    "        train_accuracies.append(accuracy_train)\n",
    "        # evaluate testing sets step-wise\n",
    "        combined_net.eval()\n",
    "        y_pred_test = combined_net(x_test_images_norm_torch.to(device),x_test_nhts_torch.to(device))\n",
    "        loss_test = criterion(y_pred_test.squeeze(), y_test_torch.to(device))\n",
    "        test_losses.append(loss_test)\n",
    "        _, predict_test = torch.max(y_pred_test.to(device), axis = 1)\n",
    "        accuracy_test = (predict_test == y_test_torch.to(device)).sum().item()/n_test\n",
    "        test_accuracies.append(accuracy_test)\n",
    "        # print info\n",
    "        if n_epoch % 1 == 0:\n",
    "            print('Epoch {}: train loss: {}; test loss: {}'.format(n_epoch, loss.item(), loss_test.item()))\n",
    "            print('Epoch {}: train accuracy: {}; test accuracy: {}'.format(n_epoch, accuracy_train, accuracy_test))\n",
    "\n",
    "# notes:\n",
    "# CPU training: about 30 mins, with SIMPLEST CNN architecture, 20 epoches and 200 batch_size. \n",
    "# training accuracy: 60%; testing accuracy: 60%.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f70300cb358>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl8VNX5+PHPM5OZ7PvClkDY90UIiKICWhHUutTWulW/Vks3q7XVqr+22tb2q61+W0trXaq4tqh1RUUFFcQqKIvsO2FJCJB9X2fm/P44AwRISEgmmZB53q/XvDJzz517n1zIc+8959xzxBiDUkqp0OEIdgBKKaU6lyZ+pZQKMZr4lVIqxGjiV0qpEKOJXymlQowmfqWUCjGa+JVSKsRo4ldKqRCjiV8ppUJMWLADaEpKSorJzMwMdhhKKXXKWLVqVaExJrU163bJxJ+ZmcnKlSuDHYZSSp0yRGRPa9fVqh6llAoxmviVUirEaOJXSqkQ0yXr+JVS3VdDQwO5ubnU1tYGO5RTUkREBOnp6bhcrjZvQxO/UqpT5ebmEhsbS2ZmJiIS7HBOKcYYioqKyM3NpX///m3ejlb1KKU6VW1tLcnJyZr020BESE5ObvfdkiZ+pVSn06TfdoE4dt0n8Xvq4LO/ws6Pgx2JUkp1ad0n8Tvd8NkcWPtysCNRSnVRpaWl/OMf/2jTdy+88EJKS0tbvf5vfvMbHn744Tbtq6N1n8QvAplnwe5PQSeQV0o14USJ3+v1nvC7CxYsICEhoSPC6nTdJvF7vD7+srMnlO+D4uxgh6OU6oLuvvtudu7cybhx47jzzjtZsmQJ06dP55prrmH06NEAXHbZZUyYMIGRI0fy5JNPHv5uZmYmhYWF7N69m+HDh/O9732PkSNHMmPGDGpqak643zVr1jB58mTGjBnD5ZdfTklJCQBz5sxhxIgRjBkzhquuugqATz75hHHjxjFu3DhOO+00KioqAn4cuk13zjCngxWMtB92fwrJA4MbkFKqRb99eyOb8soDus0RveO47+sjmyx78MEH2bBhA2vWrAFgyZIlfPnll2zYsOFw98i5c+eSlJRETU0NEydO5IorriA5Ofmo7Wzfvp158+bxz3/+kyuvvJLXXnuN6667rtmYrr/+ev72t78xdepU7r33Xn7729/yyCOP8OCDD7Jr1y7Cw8MPVyM9/PDDPProo0yZMoXKykoiIiICcViO0m2u+AHcPYZSLImw69Ngh6KUOkVMmjTpqD7xc+bMYezYsUyePJmcnBy2b99+3Hf69+/PuHHjAJgwYQK7d+9udvtlZWWUlpYydepUAG644QaWLl0KwJgxY7j22mt58cUXCQuz1+FTpkzhZz/7GXPmzKG0tPTw8kDqNlf8AAPTYvls93Au3rUUMcbW+yuluqzmrsw7U3R09OH3S5Ys4cMPP2TZsmVERUUxbdq0JvvMh4eHH37vdDpbrOppzrvvvsvSpUuZP38+999/Pxs3buTuu+/moosuYsGCBUyePJkPP/yQYcOGtWn7zWnxil9E5opIvohsaKb8WhFZ5399LiJjG5XtFpH1IrJGRDp8nOVBaTF85h2BVOVD4baO3p1S6hQTGxt7wjrzsrIyEhMTiYqKYsuWLSxfvrzd+4yPjycxMZFPP7U1ES+88AJTp07F5/ORk5PD9OnT+dOf/kRpaSmVlZXs3LmT0aNHc9ddd5GVlcWWLVvaHcOxWnPF/yzwd+D5Zsp3AVONMSUiMgt4Eji9Ufl0Y0xhu6JspYGpMTzm819B7FoKqUM7Y7dKqVNEcnIyU6ZMYdSoUcyaNYuLLrroqPKZM2fy+OOPM2bMGIYOHcrkyZMDst/nnnuOH/zgB1RXVzNgwACeeeYZvF4v1113HWVlZRhjuP3220lISODXv/41ixcvxul0MmLECGbNmhWQGBoT04qujyKSCbxjjBnVwnqJwAZjTB//591A1skm/qysLNOWiViKKuuY8PtFbI69hchRF8Olfz/pbSilOtbmzZsZPnx4sMM4pTV1DEVklTEmqzXfD3Tj7k3Ae40+G2ChiKwSkdkB3tdxkmPCSYxyU+JMhqpOuclQSqlTTsAad0VkOjbxn9Vo8RRjTJ6IpAGLRGSLMWZpM9+fDcwG6Nu3b5vjGJgaQ35JHL2r8tu8DaWU6s4CcsUvImOAp4BLjTFFh5YbY/L8P/OBN4BJzW3DGPOkMSbLGJOVmtqq+YKbNCgthtyGGKgsaPM2lFKqO2t34heRvsDrwHeMMdsaLY8WkdhD74EZQJM9gwJpYGoMeQ2xmKp8HbpBKaWa0GJVj4jMA6YBKSKSC9wHuACMMY8D9wLJwD/8w4V6/A0MPYA3/MvCgH8bY97vgN/hKAPTollu4hBPLdRXQnhsR+9SKaVOKS0mfmPM1S2U3wzc3MTybGDs8d/oWINSY3nHxNsPlfma+JVS6hjdasgGgD6JkZQ6E+2HKq3nV0od0Z5hmQEeeeQRqqurmyybNm0abemGHgzdLvE7HQLRKfaDJn6lVCMdmfhPJd0u8QOYqDT7plK7dCqljjh2WGaAhx56iIkTJzJmzBjuu+8+AKqqqrjooosYO3Yso0aN4uWXX2bOnDnk5eUxffp0pk+ffsL9zJs3j9GjRzNq1CjuuusuwI73/z//8z+MGjWK0aNH85e//AVoemjmjtatBmk7xBmTAsXoFb9SXd17d8OB9YHdZs/RMOvBJouOHZZ54cKFbN++nS+//BJjDJdccglLly6loKCA3r178+677wJ2DJ/4+Hj+/Oc/s3jxYlJSUprdfV5eHnfddRerVq0iMTGRGTNm8Oabb5KRkcG+ffvYsMF2bjw0DHNTQzN3tG55xR8XE0UpsXrFr5Q6oYULF7Jw4UJOO+00xo8fz5YtW9i+fTujR4/mww8/5K677uLTTz8lPj6+1dtcsWIF06ZNIzU1lbCwMK699lqWLl3KgAEDyM7O5ic/+Qnvv/8+cXFxQNNDM3e0bnnFnxTlptDEkaBX/Ep1bc1cmXcWYwz33HMP3//+948rW7VqFQsWLOCee+5hxowZ3Hvvva3eZlMSExNZu3YtH3zwAY8++iivvPIKc+fObXJo5o4+AXTLK/7EaDcFvni8FXrFr5Q64thhmS+44ALmzp1LZWUlAPv27SM/P5+8vDyioqK47rrruOOOO1i9enWT32/K6aefzieffEJhYSFer5d58+YxdepUCgsL8fl8XHHFFdx///2sXr262aGZO1r3vOKPdlNIHEarepRSjRw7LPNDDz3E5s2bOeOMMwCIiYnhxRdfZMeOHdx55504HA5cLhePPfYYALNnz2bWrFn06tWLxYsXN7mPXr168cADDzB9+nSMMVx44YVceumlrF27lhtvvBGfzwfAAw880OzQzB2tVcMyd7a2Dst8yAcbD5A371auj/wc5y9zAxiZUqq9dFjm9utqwzJ3CUnRbgpNPM6GCmg4fto0pZQKZd0y8SdGuSnE3wqvDbxKKXWUbpn47RW/7SqFjsuvVJfTFauYTxWBOHbdMvHHR7ooPnTFr+PyK9WlREREUFRUpMm/DYwxFBUVERER0a7tdMtePU6HUBeebCd+1KoepbqU9PR0cnNzKSjQv822iIiIID09vV3b6JaJH8BEp0IlWtWjVBfjcrno379/sMMIad2yqgcgJiaWaonSqh6llDpGt038iVFuSiVer/iVUuoY3TbxJ0W7KTBxWsevlFLH6LaJPzHazQFvHEarepRS6igtJn4RmSsi+SKyoZnya0Vknf/1uYiMbVQ2U0S2isgOEbk7kIG3JCnKTYFPx+tRSqljteaK/1lg5gnKdwFTjTFjgPuBJwFExAk8CswCRgBXi8iIdkV7EpKi3RQRj9QUg9fTWbtVSqkur8XEb4xZip3Pqrnyz40xJf6Py4FDHUwnATuMMdnGmHrgJeDSdsbbaraOPx7BQHVRZ+1WKaW6vEDX8d8EvOd/3wfIaVSW61/WKRL9A7UB2rNHKaUaCdgDXCIyHZv4zzq0qInVmn1GW0RmA7MB+vbt2+54kqLcFB0ar0fr+ZVS6rCAXPGLyBjgKeBSY8yhepVcIKPRaulAXnPbMMY8aYzJMsZkpaamtjumxGhXoxE6C9u9PaWU6i7anfhFpC/wOvAdY8y2RkUrgMEi0l9E3MBVwPz27q+1YsLDKHX4Z7LRqh6llDqsxaoeEZkHTANSRCQXuA9wARhjHgfuBZKBf4gIgMd/5e4RkVuADwAnMNcYs7FDfoum48YVGU+D141Lq3qUUuqwFhO/MebqFspvBm5upmwBsKBtobVfUkw45RUJJOvTu0opdVi3fXIXbJfOEonXYRuUUqqRbp34E/19+bVXj1JKHdGtE39SlJsD3li94ldKqUa6deJPjHaz3xOLqSoAny/Y4SilVJfQrRN/UpSLQhOP+DxQWxrscJRSqkvo3ok/JrzRsA1a3aOUUtDdE3+Um4JDT+9qA69SSgHdPPEnRruOjNejV/xKKQV088SfdNQInZr4lVIKunniT4xyU0IMPpxa1aOUUn7dOvFHuJxEul1UuRJ0oDallPLr1okfbHVPmSMRKg4GOxSllOoSQiLxFzhSoHxfsENRSqkuodsn/sQoN3kmBcpyWl5ZKaVCQLdP/EnRbvZ6k6C2DOoqgh2OUkoFXbdP/IlRbnbW+WfiKtPqHqWU6vaJPynaxa6GRPuhLDe4wSilVBcQAok/3Nbxg9bzK6UUrUj8IjJXRPJFZEMz5cNEZJmI1InIHceU7RaR9SKyRkRWBirok5EU7SKfBIw49YpfKaVo3RX/s8DME5QXA7cCDzdTPt0YM84Yk3WSsQVEYpQbL07qonpo4ldKKVqR+I0xS7HJvbnyfGPMCqAhkIEFSlK0G4Dq8J7al18ppej4On4DLBSRVSIyu4P31aREf+Ivc/fQOn6llKLjE/8UY8x4YBbwYxE5p7kVRWS2iKwUkZUFBYEbSTMh0gVAoTPVdufUKRiVUiGuQxO/MSbP/zMfeAOYdIJ1nzTGZBljslJTUwMWQ5jTQUKUiwOSCr4GHaxNKRXyOizxi0i0iMQeeg/MAJrsGdTRkqLc5PqS7Qd9iEspFeLCWlpBROYB04AUEckF7gNcAMaYx0WkJ7ASiAN8IvJTYASQArwhIof2829jzPsd8Uu0pGd8BNuq/TNxleVA+oRghKGUUl1Ci4nfGHN1C+UHgPQmisqBsW2MK6AyEqNYcTDWftAunUqpENftn9wFSE+MJLsyDOOO0cSvlAp5oZH4kyIBoSG6t3bpVEqFvNBI/IlRAFSGp0LFgSBHo5RSwRUSiT/Dn/hLHEma+JVSIS8kEn9abDgup5BvEqHygD7EpZQKaSGR+B0OoU9CJDmeePB5oKbZoYeUUqrbC4nED7aef1edv0tnxf7gBqOUUkEUMok/IymSrVW2rp+Kg8ENRimlgihkEn96YhTbqmPsB73iV0qFsBBK/JEUGP+k65Xas0cpFbpCKvHX4abBHa9dOpVSIS1kEv+hvvxV7hRN/EqpkBYyiT8lJhx3mIMSR7ImfqVUSAuZxO9wCOkJkRw0CZr4lVIhLWQSP9hx+fd546HyoD69q5QKWSGV+HvERbC3Ps5OwahP7yqlQlRIJf602HCyaw89vavVPUqp0BRSiT81Npx9Xn9ffk38SqkQFVKJPy0ugnwOJX59elcpFZpaTPwiMldE8kVkQzPlw0RkmYjUicgdx5TNFJGtIrJDRO4OVNBtlRYbrk/vKqVCXmuu+J8FZp6gvBi4FXi48UIRcQKPArOAEcDVIjKibWEGRo+4COpwU++K06oepVTIajHxG2OWYpN7c+X5xpgVQMMxRZOAHcaYbGNMPfAScGl7gm2vtNhwACrdOgWjUip0dWQdfx+g8czmuf5lQRMdHka020mxMwVK9wYzFKWUCpqOTPzSxDLT7Mois0VkpYisLCgo6LCg0uIi2OvIgKId+hCXUiokdWTizwUyGn1OB/KaW9kY86QxJssYk5WamtphQaXGhrPd1xsaqqEsp+UvKKVUN9ORiX8FMFhE+ouIG7gKmN+B+2uVtNhwNtT3sB8KtwU3GKWUCoKwllYQkXnANCBFRHKB+wAXgDHmcRHpCawE4gCfiPwUGGGMKReRW4APACcw1xizsWN+jdbrERfBguoe9pRXsBUGnx/skJRSqlO1mPiNMVe3UH4AW43TVNkCYEHbQusYabHh7K+PwpeUgqNgS7DDUUqpThdST+4CpMXZLp11CYO1qkcpFZJCL/HHRgBQFtMfCraAabajkVJKdUshmPjtFX9BeD+oLYPK/CBHpJRSnSsEE7+94s8J62sXFG4NYjRKKdX5Qi7xx0WGER7mYIfxP0RcoIlfKRVaQi7xiwhpceFk18SCO1YTv1Iq5IRc4gdb3XOwoh5Sh9oGXqWUCiEhmfjTEyPJKamGtGGa+JVSISckE3/fpCjySmvwpgyDqgKoKgx2SEop1WlCNvH7DBRGDbAL8jcHNyCllOpEIZv4AXZJP7tAE79SKoSEZOLvlxwNwI6aGIhIgPxNQY5IKaU6T0gm/rTYcNxhDvaW1EDaCL3iV0qFlJBM/A6HkJEYyd6iakgbbhO/jtmjlAoRIZn4wVb37Cn2J/66MqjYH+yQlFKqU4Rs4u+bFEVOcTUmdZhdoPX8SqkQEbKJPyMpiso6DyXRA+0CredXSoWIkE38/fxdOvfURkJMD038SqmQEbKJv2+yTfx7D9XzH1gf5IiUUqpztJj4RWSuiOSLyIZmykVE5ojIDhFZJyLjG5V5RWSN/zU/kIG3V0aiP/EXVUO/s+DAOijPC3JUSinV8Vpzxf8sMPME5bOAwf7XbOCxRmU1xphx/tclbY6yA0S6naTFhtsr/pGX2YWbutS5SSmlOkSLid8YsxQoPsEqlwLPG2s5kCAivQIVYEfqmxTF7qIqSBkMaSNh05vBDkkppTpcIOr4+wA5jT7n+pcBRIjIShFZLiKXnWgjIjLbv+7KgoKCAITVsnEZCazNKaOitgFGXAp7l0O59udXSnVvgUj80sSyQ4/B9jXGZAHXAI+IyMDmNmKMedIYk2WMyUpNTQ1AWC2bMbIn9V4fn2wr8Ff3GNj8dqfsWymlgiUQiT8XyGj0OR3IAzDGHPqZDSwBTgvA/gJmQr9EkqLdLNx40M7GlTocVjwFez7XIRyUUt1WIBL/fOB6f++eyUCZMWa/iCSKSDiAiKQAU4Au9Xis0yGcNyyNxVvyqff4YNrdUHEAnpkF//pWsMNTSqkO0ZrunPOAZcBQEckVkZtE5Aci8gP/KguAbGAH8E/gR/7lw4GVIrIWWAw8aIzpUokfbHVPRZ2HL3YV2eqen2+BSbNhxyIo2xfs8JRSKuDCWlrBGHN1C+UG+HETyz8HRrc9tM5x9uAUIl1OFm48yNmDU8EdBeOuhS+ftFU+Y/TKXynVvYTsk7uHRLicnD04hY+35B9Z2HM0hMfBns+CF5hSSnWQkE/8AGcOTGZfaQ25JdV2gcMJfSfbK36wPX1euBx83uAFqZRSAaKJH5jUPxmAL3c1ek6t35lQuBUqDsKie2Hnx7BvdZAiVEqpwNHEDwztGUtcRNgxiX+K/fnBPVCcbd/v+LDzg1NKqQDTxI/t1jkxM+noxN9rHIRFwobXIGkA9JmgiV8p1S1o4vc7fUAS2YVV5JfX2gVhbsiYZN9PuQ0GXwD7VkH1iYYtUkqprk8Tv9/hev7djRL7yMuhx2he9ZzFTZ/FA8bW9Sul1ClME7/fyN5xRLmdR1f3ZN0IP/wvy/dWsbiiD96IRK3uUUqd8jTx+7mcDib0S+SlL3OY/L8f8aN/rTpcll1QiQ8HBWlTbOL3+YIYqVJKtY8m/kZuO28wV0zoQ6+ECBasP2CHawZ2FVYBsCFmClQVwK5PghmmUkq1iyb+RrIyk3jgG2O4ZfogALYdrKCkqp6SansCWMxEiEyE1c/ZL2z7AB4ZYwd2U0qpU4Qm/iYM6xUHwOb9FWT7r/adDmFrUQOMvRo2vwOlOfDuHVC6B9b8O5jhKqXUSdHE34Te8RHERoSx5UD54WqeiZmJ9iQw/gbwNcALl0HZXohLh69e1PH7lVKnDE38TRARhvWMZcv+CnYVVhLmEKYOSaO4qp6S6AGQMRmKdtjpGs/9JRTvtNM2KqXUKUATfzOG9Yxjy4EKduZX0TcpiqE9YwDILqyEM2+B6DSY8Xub/N2x9qpfKaVOAZr4mzGsVyyVdR6WZRcxIDWagak28e8sqILhX4c7t0NCX3BHw6hvwMY3oKY0yFErpVTLNPE3Y1jPWADKahronxJNemIUbqeDnQWVx6888Wbw1MK7P9e6fqVUl6eJvxlDesQeft8/JQanQ8hMiSK7oOr4lXuNgWn3wIZXYc2/OjFKpZQ6ea1K/CIyV0TyRWRDM+UiInNEZIeIrBOR8Y3KbhCR7f7XDYEKvKPFRrjISIoEYEBqNAADU2OavuIHOPtnkHk2LLgTCrd3VphKKXXSWnvF/yww8wTls4DB/tds4DEAEUkC7gNOByYB94lIYluD7WzDetr+/ANSbOIfkBrN3qJqahuamInL4YRvPAlhEfDqjeCp68xQlVKq1VqV+I0xS4ETjUd8KfC8sZYDCSLSC7gAWGSMKTbGlACLOPEJpEs5Z3AKg9JiSI0NB2DKwBQ8PsP8tXlNfyGuN1z2GBxYb2ftUkqpLihQdfx9gJxGn3P9y5pbfkr4zhmZfPizqYgIAGcMTGZYz1jm/ncXprlG3KEz4fQfwhePw8q5nRitUkq1TqASvzSxzJxg+fEbEJktIitFZGVBQUGAwgosEeG7U/qz5UAFy3YW8UV2Efe9teH4qp/zf2snbnnndvj8b8EJVimlmhGoxJ8LZDT6nA7knWD5cYwxTxpjsowxWampqQEKK/AuGdeb5Gg3v3htHVf/cznPLdvDO+v2H71SWDh8+0U7kcvCX9mXr4l2AaWUCoJAJf75wPX+3j2TgTJjzH7gA2CGiCT6G3Vn+JedsiJcTr5zRj9yS2q4ZGxv+qdE8/KKvcevGOaGK56Gid+zV/0vfwfqm+gKqpRSnSysNSuJyDxgGpAiIrnYnjouAGPM48AC4EJgB1AN3OgvKxaR+4EV/k39zhhzyk9ae8v0QZw3rAej+sTxxNJsHnxvCzvyKxmUFnP0ig4nXPQwpAyG9++Gl66Bq18GV0RwAldKKUCabaQMoqysLLNy5cpgh9Eq+RW1nPnAx9x0Vn/uuXB48yuu+Te8+UMYMhOufN5WBymlVICIyCpjTFZr1tUnd9spLTaCc4el8eqqXOo9J5iScdw1cOHDsO19+FsWrHoOGmptmc9rR/esLeucoJVSIU0TfwBcfXpfiqrqeXPNvhOvOOl7cN3rEJMKb98KfxoA//42/GUUzL0A3r6tcwJWSoU0TfwBMG1IKqP6xPH3j3fQ4G1hIvZB58HNH8H1b8HYb0P+Zug5Gkb6R/jMW9M5QSulQpYm/gAQEX563hD2FlfzxuoWrvrtF2DANLj4L/DTdXDtK/D1R+x8vh/f39HhKqVCnCb+ADlveBqj+8Tzt8XbW77qb0pEPJz1M9jxIXzyJ1j7kk7irpTqEJr4A0REuP38weQU1/DCsj1t28ik70HqcFj8B3jj+/DEOXbcH6WUCiBN/AE0fWgaU4ek8pdF28gvrz35Dbgi4Qf/hTt2wE2LwBEGz1xop3WsKgp8wEqpkKSJP4BEhN9cMpI6j4/fvbOJ+WvzuO+tDSd3EnCG2V4/GZPgpoUQnwFv/RgeHgQvXgG7P+u4X0ApFRJa9eSuar3+KdHMPmcAf1+84/AYPiXVDcy5+rST31h8ur0DyPsKtr0HK5+BZy+EPhNg/A3QZ7xtB0joB6lDAvybKKW6K31ytwPUNniZ9+VeRveJZ/HWfB5dvJPXf3Qm4/u2cw6ahhpb7bPiKSjYcmR5VDLctg7CY5r/rlKqWzuZJ3c18XewqjoP0x5eQkZiJK/98MzDY/u3izGQuxLKc8FTD2/MhvPuhbN/3v5tK6VOSTpkQxcSHR7GnTOGsnpvKa+1po9/a4hAxkQ77PPYb9ux/z+bY4d8qCmB4uzA7Ecp1S1pHX8nuGJCOv9ZlcNv397IlEHJ9IqPDOwOpv8/eHIqvHA5HNwEnhr7NHDm2VBTCu5oezcQ1+vo7xljTyJKqZCiV/ydwOkQHvrmWDxew12vrefDTQf5w7ub2HqgIjA76D3ODvlwYAOM/iZc8L/gcNmpH3cthdXPw98nwrJH7cBwXg8s/DX8MRM+eci2HdRVQlnu0dv11AcmPqVUl6J1/J3ohWW7+fVbGw9/To528/L3JzMoLZaD5bUkRbtxOdt4LvbUg7e+6Qbe4mxYcKd9Kjimp50UPm819BoL+9eCKxoa/JPETP6xnTrys7/Ckgdg6CyY/itIG9a2uJRSnUIbd7son8/w2upceidEkhITzrVPfYFDoFd8BGtzy7hucl9+f9nojtm5MbD7UzscxL7Vdpygsd+G3f+Fda9AQoa94l/1LMT0gMqDtqoobw3UV0DqMEifCGfeql1HleqCNPGfIrYeqOC6p7+gR1w48ZEulu0s4v2fnsOQHrEdu2Ovxz4o1pRVz9khI865EybebBuLVz1r5wvY8zn4PHDBHyDru9o+oFQXoon/FGKMQUQoqarnnIcWk9UvkWdunBTssJpWccDOIrbzYxgyCy75m33KWCkVdNqd8xRyqF9/YrSbn5w7iMVbC3hnXV6Qo2pGbE+49jWY+aBN/o+dqYPIKXUKalXiF5GZIrJVRHaIyN1NlPcTkY9EZJ2ILBGR9EZlXhFZ43/ND2Tw3c31Z2QypEcMt/z7K659ajlrckqDHdLxHA6Y/EOYvRg8tfDFE8GOSCl1klpM/CLiBB4FZgEjgKtFZMQxqz0MPG+MGQP8DnigUVmNMWac/3VJgOLuliJcTubfcha/vngEW/ZXcNmjn3H7y2vYX1YT7NCO12MkDJwOOz6yDcdKqVNGa674JwE7jDHZxph64CXg0mPWGQF85H+/uIly1UoRLic3ndWfT34xnR9NG8i76/cz/eEl/HnRNirrPMEO72iDzoeKPMjfFOxIlFInoTWJvw+Q0+hzrn9ZY2uBK/x6eqP7AAAX90lEQVTvLwdiRSTZ/zlCRFaKyHIRuaxd0YaQmPAwfjFzGB/9bCpfG96DOR9tZ/z9i7h+7pc8+9ku9hRVBTtEO38w2OcDlFKnjBZ79YjIt4ALjDE3+z9/B5hkjPlJo3V6A38H+gNLsSeBkcaYMhHpbYzJE5EBwMfAecaYnU3sZzYwG6Bv374T9uxp4yxW3dSanFLeXpvH4q35ZBfYpD8gJZpzhqRy6bjenNbekT/b6h9nQnQy3PB2cPavlAIC3J1TRM4AfmOMucD/+R4AY8wDzawfA2wxxqQ3UfYs8I4x5tUT7TOUunO2xe7CKhZvzeeTbQUs21lEg9fH7V8bwo+nD8Lh6OS+9Qt/Dcsfg7t2QXgHP3+glGpWoBN/GLANOA/YB6wArjHGbGy0TgpQbIzxicgfAK8x5l4RSQSqjTF1/nWWAZcaY05YKayJv/Uq6zz86o31vLkmj7TYcGrqvYS7HJw7LI3pQ9M4rW8iPeMjOi6AXUvhua/DzD/ChBvs9JGHHPq/pQ96KdXhAv4Al4hcCDwCOIG5xpg/iMjvgJXGmPki8k1sTx6Drer5sT/Znwk8Afiw7QmPGGOebml/mvhPjjGG/6zK5b/bC0mKdlNUVc+SLflU+BuD02LDGZuRwLiMBMakx5MQ6WbRpgNszCtnYv8kzhqUQlpsOPFRLsLDnIe3W1BRR0qM+8RzCHjq4e9ZULoHnOEw7mo47z47BtA7t0N9JfSdDAPPgxGXQlSS/V7hdtjwun0AbMKNR58cindB0U4Y/LWOOFxKdUv65K6i3uNj/b4y1uWWsi63jLU5pWQXHmkQdghkJEWxp6j68DKnQ5g+NJWzBqXw1to8vtpbypAeMdxwZiYXj+lNfKSr6Z3VVcKez2DrAlj9Arii7Pg+yYOgTxbs/RxK99rJ4+N6Q301VBce+f64a+HrfwWnCwq2wrMXQVWBfVBs8g876hB1X7XlsOlNWPNve9fV93QYezWkDQ92ZKqxXUuhusjOqxEAmvhVk8qqG1i/r4z8ilrOHpxKamw4B8pqWbWnhJLqenYXVvHW2jwKKurITI7i62N789HmfDbtL8flFM4alML1Z2YybUhq83cBBzfBh7+B1KF2ngBXpE0+B9bZK/yK/fbEkDIERl5mh4xe8gCkjYCB58L6/9j1e42FHYtg+i9h+CWQPNCeGJrj89m7jor90Pu0o6ucujufD7a8DZ//DfK32JMuQMpQiIizA+25o+DG96HHsY/gqKB5cjqU7II7s+2Dke2kiV+1mcfrY1dhFQNSY3A6BGMMa3JKeW/DAeavyeNAeS1j0uO57vR+XDCqZ/N3ASdj/avw5T/tUNGRiXD9fEgaAK/eCFveses43fZk0WMk9BpnRwrtM8GWffEYLH7gSMJzx8Kwi+zcBAOmnfiEcaopzbEn1oYaiIiHqnzI3wzl++wd1qCvQUwa9D3TVrGJ2Lutp2fY73/3A0jsF9RfQQH1VfBABhgv/Gh5QO7GNPGrDlHv8fH66lyeWJrNrsIq3E4Hv7p4ONefkRmYHTTUgsN5JFEbY8cCyt8M+Rvt3cTBDfaqHiChH8T2gpzl9mGy4RdDVApsew82vQ11ZTY5JvSF8Hg750Btuf1OyiAIj7OJsWCbvSOJSIBeY2wMVYX2ZBPX245RFNsLinbA1vfskNXuGHtyGjLTVqVEJdt4y3LsH3VkEtSV2zGNKg7AkAsgPcv+LoXbbHWXz2PnTwiPs6/IBIhPt9/dvxYOrrfzJyQPsncwxdnwzk/B2wCJmXZ2tehkG8fwr8OIy2zsTcnfDHNnQlJ/uPnjo68wjbGvAFx1qlbK/gSe9w9kcNGfYeJN7d6kJn7VoYwxrM0t45EPt7FkawF/umIMV07MOG698toGIsKcuMMCnFAqDtg/nDX/goMbbZXSscNEe+rscBLb3oPKAjsfsTvaJtryPJvE66vtFVdipr2LqC2F/evs92NS7TbK9x+5kxAH9D3D3nnUVdjkXLT9xLE6XLa6pbro+DJxgPGd3O+eNhK+/YKt+jpZa1+CN74P33gKxnzryPK3boF9q+yzGNEpR5bvWmqXj7jUnlxU4Cz5o63ijEywd2lXPNXuTWriV52izuPle8+v4tPtBXx3Sn++OSGd4b3iAFi8NZ/b5n1Fr/hI5t44kT4Jp3Cde12FPdlEJR/plXRI4Q57N1JTYq+aEzLs1Xt1sW3M7jvZtmnkLLdDW/QYZdsz3DH2RNVQY+8MasvtNspybMN2j5G2naOq0PZw8tbb7fU/x9bXt4XPZ+dmrimFW1aAKwKyl8Dz/hFW0ifBDfOhZDcsfQg2vHbku0NmwuVP2EQVSMbA9kWw8mkYd409yYSCFy6Hynx7N5e7Am7f2O5uz5r4Vaepqfdy9+vreHfdfjw+Q5+ESIb2jGXx1nyGpMWSV1ZDhMvJry4azvBecfRPiW779JKq/Q4l+jNvhXPugH+eB74GO73m69+zJ626Mts19+yfwZgr7QxtSx+GnqPhO28cn/wrC+zdT1wve/fk8p+Y6ivtCS2mh534p7rYTugT18vO6LbpLTvFZ/4mQOyJ9dbVtnquO/N64I/9YOxVtgH+vTvhtnXtbnvRxK86XXFVPQvW72fZziLW5JRyxsBk7r90FDkl1dz4zAr2ldoRRuMiwpgxsifDesZSUethTHo85w3vEeToQ8wr19uk63TbO4lr/gNDZtiZ1rZ9AIPPh6EX2raNQ7a+D698xy6LSrHVYO4o2y5zsIk5GcRpq9HAnkTi020PlsNVWwIYSB0OU26zV75Pnw9TboXzf9e+3688D96/x7bbXPR/trdYWxXthEX32pNd7/G26+Wxd30na/9aeOIcuOJp2/vt8bPs3dTYq9q1WU38qkup83jZfrCSHfmVLN1ewKKNBw8/XAbwm6+P4H+m9D/8edWeEtbnlnL9GZmdPwRFKPD57PzLq5+3ifyCP7Tue9s/hM//ak8YYRHQUG2ravpNsQ3cVQVQssfO0+BtsMnSHWMTfvEuW801YKptnD+wHjImw+AZRxqV3/ghbHgVrnvNNqYbA946+5Cgt86u43Ad6QDgcPl/htmfxbvssySrnrUN57E9bbXV6G/ZHmAxafZE5fPYu4vIBBun8dqG/chEe4IKC7f7yl0F/77ySDVbTbH9fU7/Poz5tm3rMcb+PsU7beN7dKqtMjvRzHRfPAHv/cJW78T2gj/2t12bL5nT1n9RQBO/6uLqPb7DQ0vc9tJXfLDxILdMH8QPpg1k5e5ivv/CKuo8Pi4a3Yv/u3IsEa5meqqo7qV8P/xtgu191VZOt+1Bdf79NvF//Hs7j/ShBvoWiU3ePo/tEBCfDte9bhvTD26ET/8PNr4BGHui8NTZE+Cxeo6x3YnrKmGzfwDDfmfaBv0t74LDyT8nzGdfaQ2/qfydrYIbOguGXWx7ZzU3J/aJItfEr04VDV4fd726jte/2kdsRBh1DT4GpcUwc1RP/rxoG6f1TeDBb4xhaE8dAK616j22OiXgvak6Q8luezfQUGsbO51uewXudNvPXo9tk/A22J8+75H3kUl2cqBjBws0xvaqqiqw3WLFaT/Xlh65g6gtt0+Tl+61z0Q43TaxT5pt7xSOjXHXp5D7pX1mJHmgfSX2t9VMOcth03z7XIo47B1RWLht3zAG0rMwE27krHfiOVBey6ofDyZh9d9hywJ759LGhl5N/OqUsyanlH8uzaayzsOcq04jPsrFu+v28//eWE9lnYdvTUhnQr9EUmLD2ZlfSVlNAxeN6cWwnnEYYyiuqqeqzovXGDKTo048vlA3lV1QyfPL9vDqqlwq6zwkRbsJD3PQ4DWcNyyN318+ShvWO1PpXgiLPFLt4/O3eTic7Cyo5Lz/+wTgSHdonxfKctvcyKuJX3UbxVX1PLxwK6+tyqXOc6TPu0PAZ2BgajT5FXVU1B5pM0hPjOSiMb2YNiSN8f0Sjhp47lSXU1zNz19Zy8C0aL6VlUFmcjSVtR4e+2QnL6/Yi9MhXDS6F/1TYjhYUUuDx0d1g5d31+3na8PT+Ps144+rOvtsRyHFVfXMGtWTMD0xdIpnP9vFb97eRHyki/F9E3jmxknt3qYmftXteLw+9hZXU1hZz8DUaBwivLoql//uKCQjKZIBKTHER7qo8/hYuOkAn24vxOszuJ0OkmPcxEaE4fUZvD5D74RIhvSIJS7Shcsh5JXVsLOgikiXkz6JkRhjKK1uwOX/bnK0m6TocHJLqvl8ZxHFVfXERYbRJyGSCf0SGZQWQ0SYE4/PUFRVh0OE8X0TCQ9z8MHGA+wpqmZi/ySG94wjt7Sagoo63E4HEW4nEWFOotxOYiPCSIhykxjlQkSorPOweX85PWIj6JMYidMh5BRXc9WTyymracDrM9Q0eA8fnzCHcN3kfvx4+iBSY8OPO34vLNvNr9/aSGZyFBeP6c3I3nHUeXy8tjqXT7fbAfMGp8Vwy7mDOGNAMoWV9fz1o22s3lvKoNQYxmYkcPlpfbTKLUBufm4F2w5WMmNED55ftoeVv/4acRHtG1pEE78KeeW1DXyZXcyKPcUUV9ZTUevB6RAcDmFvcTU7DlZQVW8TZ2KUi4GpMdR5fOwrrcEhQnxkmE3klfWH5zoOcwjjMhLokxhJeU0D2YVVR41u2hyXU2jwtu7vLD7SRc+4CHYUVOL12e+4nQ7io1zU1ntxOIR/3Xw6/ZKj+HDzQcqqG3A4hLMHp9I/JfqE21606SDPfr6L5dnFh7cdH+niJ+cOondCJH96fwu7G/0+sRFhnDssjd1F1WzKK6PBaxibkcCVWenMGNGTspp6f0+tQrbsL+eMgcl8bXgPHCKU1zZQUeuhqs5DSkw4KTHh7CioYFNeORlJUUzMTCImPIw6j4+CijoKKus4Y0Bykyet7qbB62Pcbxdy2Wl9+Mb4Plzx2DL+etU4Lh137Iy2J0cTv1KtYIyhwWtabAStbfBSXFVPfKSL6PCje1vkV9Syr6SGeo8Pp0NIinZT0+Bl9d5SymsaOH9EDzKTo1m9t4RdhVVkJEbRMz4Cj8/2bKpt8FHT4KGi1kNhZT3ZBZXkldYwsnc84zISKKqqI7uwirLqBuq9Pm46qz8je7fvAaeSqnr2l9US4XLQMz6CKLf9nRq8dijv1XtK8PoMV03sS3yUvQotqqzjzTV5vLIih60Hj+4hExMexsDUaNbvK8PXQjo50UmwT0IkL82eTEZSG59M9jPGMH9tHhv2lfGT8wa3+0raGBPQNqMvdxVz5RPLePy6CcwY0YPTH/iIYT1jefqGie1qkNfEr5TqEMYY1u8r44vsYlJjw8lIimJMejwup4Oiyjq+2FVMeJiDuEgXcREuIl1OCirryC+vpX9qNIPTYskrrWHVnhLqvT7CwxwkR4fjNYZb531FtNvJ7ecPIdLtxBjbQykmwlar1TR42ZFfSU29l7hIF/H+V5TbtlnUeXwUVdbx7y/3smRrAQAZSZH8/rLRDPDfDW09UMGB8loyk6PpnRBBYWU9xVX1JMe4SYkJ59BjI8ZAcXU9L3+Zw1tr9+HxGqLcTgakxjCqTxyjesczonccDV5Dfnkt1fVevD5DYrSbvklRZCRFEuUOo6Cijo82H/TPiteD8DAH/7tgMy8u38NX984gPtLFIx9u45EPt5OZHMU9Fw5nxogebTrRaOJXSp1yNuWV852nv6Coqr5d24lyO7nzgqGM6hPP7S+vIbekps3binQ5uWRsb5Jj3FTUeth20FZXNX4AsTnJ0W6Kq+sPz0DqcgoOEeo8PqYNTeVZf4OuMYYl2wr4w7ubKa9p4JM7pxPpPvkOCZr4lVKnpJp6L/kVtdR5fAj2WYSKWg+5JTWEhzkYlBZDTHgY5bUNlNU0UF7jobreg/Gvmxztpl9S9OEqqoraBj7fWURZdQNeYxjSI4ae8ZHsKaziQHktqbHhJEa5Ka6qp7CyDmOOdKF3OR2cMzj18LYO8fkMOSXVbN5fTrjLSY/YCKLDnThEKKqqZ29xNTnF1ewtqqZnfASzRvekpt7L+xsO4PEZzhyYzJkDU45L7h6vj91F1QxKi2nTseuIOXdnAn/Fzrn7lDHmwWPK+wFzgVSgGLjOGJPrL7sB+JV/1d8bY55raX+a+JVS6uScTOJvsSVBRJzAo8AsYARwtYgcO3/bw8DzxpgxwO+wE68jIknAfcDpwCTgPhFJbO0vopRSKvBa04Q8CdhhjMk2xtQDLwHHDpo9AvjI/35xo/ILgEXGmGJjTAmwCJjZ/rCVUkq1VWsSfx8gp9HnXP+yxtYCV/jfXw7EikhyK7+rlFKqE7Um8TfVr+jYhoE7gKki8hUwFdgHeFr5XbsTkdkislJEVhYUFLQiLKWUUm3RmsSfCzSeUDUdyGu8gjEmzxjzDWPMacAv/cvKWvPdRtt40hiTZYzJSk09wVjWSiml2qU1iX8FMFhE+ouIG7gKmN94BRFJEZFD27oH28MH4ANghogk+ht1Z/iXKaWUCpIWE78xxgPcgk3Ym4FXjDEbReR3InKJf7VpwFYR2Qb0AP7g/24xcD/25LEC+J1/mVJKqSDRB7iUUqobOOWf3BWRAmBPG7+eAhQGMJyOoDG2X1ePDzTGQNEYW6efMaZVDaRdMvG3h4isbO1ZL1g0xvbr6vGBxhgoGmPg6XQ7SikVYjTxK6VUiOmOif/JYAfQChpj+3X1+EBjDBSNMcC6XR2/UkqpE+uOV/xKKaVOoNskfhGZKSJbRWSHiNwd7HgARCRDRBaLyGYR2Sgit/mXJ4nIIhHZ7v8Z9KGqRcQpIl+JyDv+z/1F5At/jC/7n9oOZnwJIvKqiGzxH88zutpxFJHb/f/OG0RknohEBPs4ishcEckXkQ2NljV53MSa4/8bWici44MY40P+f+t1IvKGiCQ0KrvHH+NWEbkgGPE1KrtDRIyIpPg/B+UYnqxukfhbOWdAMHiAnxtjhgOTgR/747ob+MgYMxg7nHVXOFHdhn0y+5A/An/xx1gC3BSUqI74K/C+MWYYMBYba5c5jiLSB7gVyDLGjMJOWnQVwT+Oz3L8UOjNHbdZwGD/azbwWBBjXASM8s/xsQ07FAz+v5+rgJH+7/zD//ff2fEhIhnA+cDeRouDdQxPjjHmlH8BZwAfNPp8D3BPsONqIs63sP9RtgK9/Mt6AVuDHFc6NgGcC7yDHVW1EAhr6vgGIb44YBf+NqlGy7vMceTIEORJQJj/OF7QFY4jkAlsaOm4AU8AVze1XmfHeEzZ5cC//O+P+tvGDiVzRjDiA17FXoTsBlKCfQxP5tUtrvg5Bcb9F5FM4DTgC6CHMWY/gP9nWvAiA+AR4BeAz/85GSg1dpwmCP7xHAAUAM/4q6OeEpFoutBxNMbsw85EtxfYD5QBq+hax/GQ5o5bV/07+i7wnv99l4jRP07ZPmPM2mOKukR8Lekuib/V4/4Hg4jEAK8BPzXGlAc7nsZE5GIg3xizqvHiJlYN5vEMA8YDjxk79HcVXaN67DB/PfmlQH+gNxCNve0/Vpf5f9mErvbvjoj8Eltl+q9Di5pYrVNjFJEo7PDz9zZV3MSyLvdv3l0Sf6vH/e9sIuLCJv1/GWNe9y8+KCK9/OW9gPxgxQdMAS4Rkd3YaTXPxd4BJIhImH+dYB/PXCDXGPOF//Or2BNBVzqOXwN2GWMKjDENwOvAmXSt43hIc8etS/0dicgNwMXAtcZfb0LXiHEg9gS/1v93kw6sFpGeXSS+FnWXxN/inAHBICICPA1sNsb8uVHRfOAG//sbsHX/QWGMuccYk26MycQet4+NMddi507+pn+1YMd4AMgRkaH+RecBm+hCxxFbxTNZRKL8/+6HYuwyx7GR5o7bfOB6f8+UyUDZoSqhziYiM4G7gEuMMdWNiuYDV4lIuIj0xzaiftmZsRlj1htj0owxmf6/m1xgvP//aZc5hicU7EaGADa+XIht/d8J/DLY8fhjOgt7m7cOWON/XYitQ/8I2O7/mRTsWP3xTgPe8b8fgP2D2gH8BwgPcmzjgJX+Y/kmkNjVjiPwW2ALsAF4AQgP9nEE5mHbHBqwCeqm5o4btpriUf/f0HpsD6VgxbgDW1d+6O/m8Ubr/9If41ZgVjDiO6Z8N0cad4NyDE/2pU/uKqVUiOkuVT1KKaVaSRO/UkqFGE38SikVYjTxK6VUiNHEr5RSIUYTv1JKhRhN/EopFWI08SulVIj5/zHoCOK0I/NFAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses, label = \"train loss\")\n",
    "plt.plot(test_losses, label = \"test loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6e44372ef0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_accuracies, label = \"train accuracy\")\n",
    "plt.plot(test_accuracies, label = \"test accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(combined_net.state_dict(), \"data/combined_net_2conv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
